{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 7: Distributional Semantics\n",
    "\n",
    "This week there is only one Jupyter notebook for you to complete!  \n",
    "\n",
    "In the lectures, we have introduced the idea of distributional semantics. In a distributional model of meaning, words are represented in terms of their co-occurrences.\n",
    "\n",
    "However, what does it mean for two words to co-occur together?  Here we are going to look at how the **definition of co-occurrence** used affects the nature of the similarity discovered.  In particular, we are going to contrast *close proximity* co-occurrence (where words co-occur, say, next to each other) with more *distant proximity* (where words co-occur, say, within a window of 10 words).\n",
    "\n",
    "First, however, we need a corpus.  Here, we are going to work with the Reuters sports corpus.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "#preliminary imports\n",
    "import sys\n",
    "import random\n",
    "import operator\n",
    "from Week4Labs.utils import normalise\n",
    "\n",
    "sys.path.append(r'T:\\Departments\\Informatics\\LanguageEngineering') \n",
    "sys.path.append(r'\\\\ad.susx.ac.uk\\ITS\\TeachingResources\\Departments\\Informatics\\LanguageEngineering\\resources')\n",
    "sys.path.append(r'/Users/juliewe/resources')\n",
    "\n",
    "from sussex_nltk.corpus_readers import ReutersCorpusReader\n",
    "\n",
    "#make sure you append the path to where your utils.py file is\n",
    "sys.path.append(r'/Users/juliewe/Documents/teaching/NLE/NLE2019/w4/Week4Labs/')\n",
    "from utils import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we set up a corpus reader for the sport category of Reuters.  Using the `enumerate_sents()` method we can see it contains over 1 million sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "1113359"
     },
     "metadata": {},
     "output_type": "execute_result",
     "execution_count": 3
    }
   ],
   "source": [
    "rcr = ReutersCorpusReader().sport()\n",
    "rcr.enumerate_sents()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to take a sample of this corpus, tokenize the sentences and carry out text normalization for case and number.  You could increase the samplesize to 10000 (which will be repeated 100 times for a total corpus size of 1000000 sentences) but this will make a noticeable slow-down in the speed of running cells.  Also note, that repeating 100 samples of size 2000 might contain duplicate items.  Don't worry about this - sampling \"with replacement\" is quite common and allows us to bootstrap estimates of statistics.  However, here, we are doing it because it is faster and so that we can check on progress with a \"completed %\" statement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "Completed 0%\n",
      "Completed 1%\n",
      "Completed 2%\n",
      "Completed 3%\n",
      "Completed 4%\n",
      "Completed 5%\n",
      "Completed 6%\n",
      "Completed 7%\n",
      "Completed 8%\n",
      "Completed 9%\n",
      "Completed 10%\n",
      "Completed 11%\n",
      "Completed 12%\n",
      "Completed 13%\n",
      "Completed 14%\n",
      "Completed 15%\n",
      "Completed 16%\n",
      "Completed 17%\n",
      "Completed 18%\n",
      "Completed 19%\n",
      "Completed 20%\n",
      "Completed 21%\n",
      "Completed 22%\n",
      "Completed 23%\n",
      "Completed 24%\n",
      "Completed 25%\n",
      "Completed 26%\n",
      "Completed 27%\n",
      "Completed 28%\n",
      "Completed 29%\n",
      "Completed 30%\n",
      "Completed 31%\n",
      "Completed 32%\n",
      "Completed 33%\n",
      "Completed 34%\n",
      "Completed 35%\n",
      "Completed 36%\n",
      "Completed 37%\n",
      "Completed 38%\n",
      "Completed 39%\n",
      "Completed 40%\n",
      "Completed 41%\n",
      "Completed 42%\n",
      "Completed 43%\n",
      "Completed 44%\n",
      "Completed 45%\n",
      "Completed 46%\n",
      "Completed 47%\n",
      "Completed 48%\n",
      "Completed 49%\n",
      "Completed 50%\n",
      "Completed 51%\n",
      "Completed 52%\n",
      "Completed 53%\n",
      "Completed 54%\n",
      "Completed 55%\n",
      "Completed 56%\n",
      "Completed 57%\n",
      "Completed 58%\n",
      "Completed 59%\n",
      "Completed 60%\n",
      "Completed 61%\n",
      "Completed 62%\n",
      "Completed 63%\n",
      "Completed 64%\n",
      "Completed 65%\n",
      "Completed 66%\n",
      "Completed 67%\n",
      "Completed 68%\n",
      "Completed 69%\n",
      "Completed 70%\n",
      "Completed 71%\n",
      "Completed 72%\n",
      "Completed 73%\n",
      "Completed 74%\n",
      "Completed 75%\n",
      "Completed 76%\n",
      "Completed 77%\n",
      "Completed 78%\n",
      "Completed 79%\n",
      "Completed 80%\n",
      "Completed 81%\n",
      "Completed 82%\n",
      "Completed 83%\n",
      "Completed 84%\n",
      "Completed 85%\n",
      "Completed 86%\n",
      "Completed 87%\n",
      "Completed 88%\n",
      "Completed 89%\n",
      "Completed 90%\n",
      "Completed 91%\n",
      "Completed 92%\n",
      "Completed 93%\n",
      "Completed 94%\n",
      "Completed 95%\n",
      "Completed 96%\n",
      "Completed 97%\n",
      "Completed 98%\n",
      "Completed 99%\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "random.seed(37)  #this will ensure it is the same sample every time you run the cell\n",
    "samplesize=2000\n",
    "iterations =100\n",
    "sentences=[]\n",
    "for i in range(0,iterations):\n",
    "    sentences+=[normalise(sent) for sent in rcr.sample_sents(samplesize=samplesize)]\n",
    "    print(\"Completed {}%\".format(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "['``',\n 'but',\n 'fortunately',\n ',',\n 'stairs',\n 'hit',\n 'the',\n 'ball',\n 'hard',\n 'and',\n 'i',\n 'charged',\n 'it',\n 'hard',\n 'and',\n 'got',\n 'rid',\n 'of',\n 'it',\n 'as',\n 'quickly',\n 'as',\n 'i',\n 'could',\n 'and',\n 'made',\n 'a',\n 'good',\n 'throw',\n '.']"
     },
     "metadata": {},
     "output_type": "execute_result",
     "execution_count": 12
    }
   ],
   "source": [
    "sentences[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1\n",
    "* Write (or adapt from previous labs) a function to find the frequency distribution of words in the sample of sentences\n",
    "* Generate a list of the 100 most frequent words in the corpus. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generating feature representations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to be able to consider any words that are in a certain **window** around a target word as features of that word.  The code below demonstrates how to iterate through a sentence and find all of the tokens within a given window of each word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens=word_tokenize(\"the moon is blue and made of cheese\")\n",
    "\n",
    "window=2\n",
    "\n",
    "\n",
    "for i,word in enumerate(tokens):\n",
    "    print(word,tokens[max(0,i-window):i]+tokens[i+1:i+window+1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2.1\n",
    "Write a function `generate_features(sentences,window=1)` which takes\n",
    "* a list of sentences (where each sentence is a list of tokens); and\n",
    "* and a window size; \n",
    "\n",
    "This function should output\n",
    "* a dictionary of dictionaries\n",
    "\n",
    "The key to the outermost dictionary is a word.  The key to each internal dictionary is a another word (a co-occurrence feature).  The value in the internal dictionary should be the number of the times the words co-occur together (within the given window).\n",
    "\n",
    "For example, with the sentences in `sents`, your function should generate the following output:\n",
    "\n",
    "<img src=\"files/output21.png\">\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pointwise Mutual Information (PMI)\n",
    "So far, we have calculated the frequency of two events occurring together.  For example, we can see how often the word 'tennis' appears in the window around the word 'player'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reps=generate_features(sentences,window=1)\n",
    "reps['player']['tennis']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use positive pointwise mutual information (PPMI) to establish how **significant** a given frequency of co-occurrence is.  If player and tennis are both very common words then their co-occurring together 10 times may be insignificant.  However, if they are rare words, then a co-occurrence of 10 should be considered as more important in the representation of each word.  PMI can be calculated as follows:\n",
    "\n",
    "\\begin{eqnarray*}\n",
    "PMI(word,feat) = log_2(\\frac{\\mbox{freq}(word,feat) \\times \\Sigma_{w*,f*} \\mbox{freq}(w*,f*)}{\\Sigma_{f*} \\mbox{freq}(word,f*) \\times \\Sigma_{w*} \\mbox{freq}(w*,feat)})\n",
    "\\end{eqnarray*}\n",
    "\n",
    "\n",
    "\n",
    "In order to carry out this calculation, we can see that we need the frequency of the co-occurrence *player* and *tennis*, the total number of times *player* has occurred with any feature, the total number of times *tennis* has occurred as a feature and the grand total of all possible co-occurrences.  We can keep track of these totals as we build the feature representations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 3.1\n",
    "Create a class `word_vectors`.  This should be initialised with a list of sentences and a desired window size.  On initialisation, the feature representations of all words, together with word totals and feature totals should be generated and stored in the object as\n",
    "* self.reps (the feature representations: a dictionary of dictionaries}\n",
    "* self.wordtotals (the frequency of each word: a dictionary of integers (with the same keys as self.reps)\n",
    "* self.feattotals (the frequency of each feature: a dictionary of integers (with the same keys as the dictionaries indexed by self.reps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate vectors from the sample sentences with a window_size of 3.  If you look at the representation of `player`, you should find that the feature `australian` has the value 17.  The total frequency of features for the word `player` is 2722, and the total frequency of occurrences of the feature `australian` is 2220"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Positive PMI (PPMI)\n",
    "We now want to convert the representation of each word from a representation based on frequency to one based on PMI.  In fact, we want to ignore any features so we use **positive PMI**\n",
    "\n",
    "\\begin{eqnarray*}\n",
    "\\mbox{PPMI}(word,feat)=\n",
    "\\begin{cases}PMI(word,feat),& \\mbox{if PMI}(word,feat)>0\\\\\n",
    "=0,& \\mbox{otherwise}\n",
    "\\end{cases}\n",
    "\\end{eqnarray*}\n",
    "\n",
    "### Exercise 3.2\n",
    "Now add a method to your `word_vectors` class which will calculate the PPMI value for each feature in each vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The PPMI between `player` and `australian` should be 3.49"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word Similarity\n",
    "We are going to use cosine similarity to compute the similarity between two word vectors.  \n",
    "\n",
    "First lets define a function to compute the dot product of two vectors. This could be imported or copied from Lab_4_2.  However, an implementation is given to you below which you can use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dot(vecA,vecB):\n",
    "    the_sum=0\n",
    "    for (key,value) in vecA.items():\n",
    "        the_sum+=value*vecB.get(key,0)\n",
    "    return the_sum"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Exercise 4.1\n",
    "\n",
    "* Add a `similarity` method to your word_vectors class to enable you to calculate the similarity between two word representations.\n",
    "* You should find similarity between `australian` and `african` is 0.05"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Nearest Neighbours\n",
    "We now want to be able to find the nearest neighbours of a given word.  In order to do this we need to find its similarity with every other word in a set of *candidates* and then rank them by similarity.\n",
    "\n",
    "### Exercise 4.2\n",
    "* Add functionality to your `word_vectors` class to be enable you to find the *k* nearest neighbours of any words.   You can improve efficiency by only considering the 1000 most frequent words as *candidates*\n",
    "* Use your functionality investigate the effect of increasing the window size on the neighbourhood of a word.  You should consider at least:\n",
    "    * the words \\['australian', 'football'\\]\n",
    "    * the neighbourhood sizes: window = \\[1, 10\\]\n",
    "* Comment on the differences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "source": [],
    "metadata": {
     "collapsed": false
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}