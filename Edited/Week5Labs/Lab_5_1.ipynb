{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 5: Lexical Semantics\n",
    "\n",
    "This week we turn our attention to lexical semantics, i.e., the meaning of words.  In this lab, you will be\n",
    "* exploring the WordNet resource\n",
    "* learning about lexical relations such as synonymy and hyponymy\n",
    "* quantifying semantic similarity via distance in the WordNet hierarchy\n",
    "* comparing WordNet similarity scores with human synonymy judgements\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, lets import WordNet from the nltk library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "#it may be necessary to run the nltk downloader, depending on what you have previously downloaded\n",
    "#if this cell fails when you run it, then uncomment and run the following lines\n",
    "#import nltk\n",
    "#nltk.download()\n",
    "import pandas as pd\n",
    "\n",
    "from nltk.corpus import wordnet as wn\n",
    "from nltk.corpus import wordnet_ic as wn_ic\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Navigating WordNet\n",
    "\n",
    "Central to the organisation of WordNet is the idea of a synset.  Words have senses and senses are grouped with synonymous senses (of other words) in **synsets**\n",
    "\n",
    "If you want to find out which synsets a word belongs to, you use the `synsets` function.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "[Synset('book.n.01'),\n Synset('book.n.02'),\n Synset('record.n.05'),\n Synset('script.n.01'),\n Synset('ledger.n.01'),\n Synset('book.n.06'),\n Synset('book.n.07'),\n Synset('koran.n.01'),\n Synset('bible.n.01'),\n Synset('book.n.10'),\n Synset('book.n.11'),\n Synset('book.v.01'),\n Synset('reserve.v.04'),\n Synset('book.v.03'),\n Synset('book.v.04')]"
     },
     "metadata": {},
     "output_type": "execute_result",
     "execution_count": 4
    }
   ],
   "source": [
    "wn.synsets(\"book\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output is a list of `Synset` objects each of which has a unique identifier containing one of its words, its part of speech and a number.  `Synset('book.n.01')` is the first noun sense of *book*.  However the word book is also in `Synset('record.n.05')` which is the fifth noun sense of *record*.  Lets inspect this synset further."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "['record', 'record_book', 'book']\na compilation of the known facts regarding something or someone\n[\"Al Smith used to say, `Let's look at the record'\", 'his name is in all the record books']\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "book_synsets=wn.synsets('book')\n",
    "recordn5=book_synsets[2]\n",
    "print(recordn5.lemma_names())  #get the words in the synset\n",
    "print(recordn5.definition())   #get the definition of the synset\n",
    "print(recordn5.examples())  #get examples of the words used in this sense"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you only want to find synsets associated with a particular part of speech of a word then you can give `synsets` an extra argument"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "[Synset('red.s.01'), Synset('crimson.s.02'), Synset('crimson.s.03')]\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "parts_of_speech={\"noun\":wn.NOUN,\"verb\":wn.VERB,\"adjective\":wn.ADJ,\"adverb\":wn.ADV}\n",
    "\n",
    "key=\"adjective\"\n",
    "print(wn.synsets(\"red\",parts_of_speech[key]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1.1\n",
    "* Write code to compute the number of synsets of each part of speech (noun, verb, adjective and adverb) for each of the following words:- book, chicken, counter, twig, fast, plant\n",
    "* Store and display the information using a Pandas dataframe\n",
    "\n",
    "Hint: you could use a nested list comprehension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "         noun  verb  adjective  adverb\nbook       11     4          0       0\nchicken     4     0          1       0\ncounter     9     2          1       1\ntwig        1     2          0       0\nfast        1     2         10       2\nplant       4     6          0       0\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "parts_of_speech_list=[\"noun\",\"verb\",\"adjective\",\"adverb\"]\n",
    "target_words=[\"book\",\"chicken\",\"counter\",\"twig\",\"fast\",\"plant\"]\n",
    "data_zip_list=[]\n",
    "for j in target_words:\n",
    "    synsets_distribution=[]\n",
    "    for i in parts_of_speech:\n",
    "        synsets_distribution.append(len(wn.synsets(j,parts_of_speech[i])))\n",
    "    data_zip_list.append(synsets_distribution)\n",
    "synsets_table=pd.DataFrame(data_zip_list,index=target_words,columns=parts_of_speech_list)\n",
    "print(synsets_table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `Synset` object also has `hyponyms` and `hypernyms` methods which return hyponym and hypernym synsets respectively.\n",
    "\n",
    "For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "[Synset('card.n.08'), Synset('logbook.n.01'), Synset('won-lost_record.n.01')]"
     },
     "metadata": {},
     "output_type": "execute_result",
     "execution_count": 8
    }
   ],
   "source": [
    "recordn5.hyponyms()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "[Synset('fact.n.02')]"
     },
     "metadata": {},
     "output_type": "execute_result",
     "execution_count": 9
    }
   ],
   "source": [
    "recordn5.hypernyms()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the hyponymy relation forms a tree, we would expect synsets to generally have multiple hyponyms and a single hypernym.  At the top of the tree (also called the **root**), the hypernym list will be empty.  Most noun concepts in WordNet share a common root hypernym which is *entity*.  At the bottom of the tree (also referred to as the **leaves** of the tree), the hyponym list will be empty"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1.2\n",
    "Write a function, `distance_to_root` that will take a Synset and traverse up the tree until it reaches a root of the tree.  When it does so, it should return the number of steps taken.\n",
    "\n",
    "Hint: This can be done using **recursion**, where a function repeatedly calls itself.  You need to define:\n",
    "* a base case:  How will the function know when it is at the top of the tree and what should it return?\n",
    "* a recursive step: In the general case, the function should call itself with a simpler problem (a Synset which is closer to the top of the tree).  When it gets the result of this function call, it needs to modify it in some way and then return its own answer\n",
    "\n",
    "Make sure you test your function.  You should find that the 5th noun sense of record is 6 steps from the top.\n",
    "\n",
    "How far are all of the other noun sense of book from a root of the tree?\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "['tiger']\n['person', 'individual', 'someone', 'somebody', 'mortal', 'soul']\nWarning: multiple hypernyms\n['causal_agent', 'cause', 'causal_agency']\n['physical_entity']\n['entity']\n4\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "def distance_to_root(asynset):\n",
    "    print(asynset.lemma_names())\n",
    "    hypernyms=asynset.hypernyms()\n",
    "    if len(hypernyms)==0: #reached the top and have to stop\n",
    "        return 0\n",
    "    else:\n",
    "        if len(hypernyms)>1:\n",
    "            print(\"Warning: multiple hypernyms\")\n",
    "        return(distance_to_root(hypernyms[0])+1)\n",
    "\n",
    "wops=wn.synsets(\"tiger\")\n",
    "print(distance_to_root(wops[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "13\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "#test\n",
    "def distance_to_leaf(synsetlist):\n",
    "    if len(synsetlist)==0:\n",
    "        return 0\n",
    "    else:\n",
    "        hyponyms=[asynset.hypernyms() for asynset in synsetlist]\n",
    "        return min([distance_to_leaf(hyponymlist) for hyponymlist in hyponyms])+1\n",
    "\n",
    "print(distance_to_leaf(wn.synsets(\"feline\",wn.NOUN)))\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Semantic Similarity in WordNet\n",
    "\n",
    "The simplest way of defining how similar two concepts are according to WordNet is to use the pathlength measure:\n",
    "\n",
    "\\begin{eqnarray*}\n",
    "\\mbox{sim}(\\mbox{synsetA},\\mbox{synsetB})=\\frac{1}{1+\\mbox{lengthOfPath}(\\mbox{synsetA},\\mbox{synsetB})}\n",
    "\\end{eqnarray*}\n",
    "\n",
    "We have also introduced other measures in the lectures which incorporate **information content**, i.e., the amount of information we receive when a word from a given synset is used (there is more information in being told that something is a *poodle* than in being told it is an *animal*).\n",
    "\n",
    "The `nltk.wn` module has built-in functions for computing these similarities between synsets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "path_similarity 0.2\n",
      "resnik_similarity 5.454686565783099\nlin_similarity 0.7098990245459575\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "books=wn.synsets(\"book\",wn.NOUN)\n",
    "print(\"path_similarity {}\".format(wn.path_similarity(books[0],books[1])))\n",
    "\n",
    "brown_ic=wn_ic.ic(\"ic-brown.dat\")  #this gets information content data from the Brown corpus\n",
    "print(\"resnik_similarity {}\".format(wn.res_similarity(books[0],books[1],brown_ic)))\n",
    "print(\"lin_similarity {}\".format(wn.lin_similarity(books[0],books[1],brown_ic)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note it is impossible to compare synsets of different parts of speech using these methods because they are not connected via hyponymy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "'\\nbooksN=wn.synsets(\"book\",wn.NOUN)\\nbooksV=wn.synsets(\"book\",wn.VERB)\\nprint(\"path_similarity {}\".format(wn.path_similarity(booksN[0],booksV[1])))\\nprint(\"resnik_similarity {}\".format(wn.res_similarity(booksN[0],booksV[1],brown_ic)))\\nprint(\"lin_similarity {}\".format(wn.lin_similarity(booksN[0],booksV[1],brown_ic)))\\n'"
     },
     "metadata": {},
     "output_type": "execute_result",
     "execution_count": 13
    }
   ],
   "source": [
    "'''\n",
    "booksN=wn.synsets(\"book\",wn.NOUN)\n",
    "booksV=wn.synsets(\"book\",wn.VERB)\n",
    "print(\"path_similarity {}\".format(wn.path_similarity(booksN[0],booksV[1])))\n",
    "print(\"resnik_similarity {}\".format(wn.res_similarity(booksN[0],booksV[1],brown_ic)))\n",
    "print(\"lin_similarity {}\".format(wn.lin_similarity(booksN[0],booksV[1],brown_ic)))\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2.1\n",
    "\n",
    "The similarity of two **words** with a given part of speech is defined as the **maximum** similarity of all possible sense pairings.  If word A has 5 noun senses and word B has 4 noun senses than there are 20 possible sense pairings to check.\n",
    "\n",
    "* Write a function which will compute the path_similarity of two nouns.\n",
    "* Make sure you test it.  The correct answer for *chicken* and *car* is 0.0909 to 3SF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "0.09090909090909091"
     },
     "metadata": {},
     "output_type": "execute_result",
     "execution_count": 14
    }
   ],
   "source": [
    "def compute_path_similarity(noun1,noun2):\n",
    "    noun1N=wn.synsets(noun1,wn.NOUN)\n",
    "    noun2N=wn.synsets(noun2,wn.NOUN)\n",
    "    maxsim=0\n",
    "    for i in noun1N:\n",
    "        for j in noun2N:\n",
    "            sim=wn.path_similarity(i,j)\n",
    "            if sim > maxsim:\n",
    "                maxsim=sim\n",
    "    return maxsim\n",
    "compute_path_similarity(\"chicken\",\"car\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2.2\n",
    "Generalise your path_similarity function so that it takes an extra optional argument:\n",
    "* the similarity measure to use\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "def word_similarity(wordA,wordB,measure,pos=wn.NOUN):\n",
    "    synsetsA=wn.synsets(wordA,pos)\n",
    "    synsetsB=wn.synsets(wordB,pos)\n",
    "    maxsofar=0\n",
    "    brown_ic=wn_ic.ic(\"ic-brown.dat\")\n",
    "    for synsetA in synsetsA:\n",
    "        for synsetB in synsetsB:\n",
    "            if measure==\"path\":\n",
    "                sim=wn.path_similarity(synsetA,synsetB)\n",
    "            elif measure==\"res\":\n",
    "                sim=wn.res_similarity(synsetA,synsetB,brown_ic)\n",
    "            elif measure==\"lin\":\n",
    "                sim=wn.lin_similarity(synsetA,synsetB,brown_ic)\n",
    "            \n",
    "            if sim>maxsofar:\n",
    "                maxsofar=sim\n",
    "    return maxsofar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "0.17900106582025765"
     },
     "metadata": {},
     "output_type": "execute_result",
     "execution_count": 16
    }
   ],
   "source": [
    "word_similarity(\"chicken\",\"car\",\"lin\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparing WordNet Similarities with Human Synonymy Judgements\n",
    "\n",
    "The file `mcdata.csv` contains human synonymy judgements for a list of 30 noun pairs.   We can read in a `.csv` file using the `csv` library "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "       word1       word2 human similarity\n0     asylum    madhouse             3.61\n1       bird        cock             3.05\n2       bird       crane             2.97\n3        boy         lad             3.76\n4    brother        monk             2.82\n5        car  automobile             3.92\n6   cemetery    woodland             0.95\n7      chord       smile             0.13\n8      coast      forest             0.42\n9      coast        hill             0.87\n10     coast       shore              3.7\n11     crane   implement             1.68\n12      food       fruit             3.08\n13      food     rooster             0.89\n14    forest   graveyard             0.84\n15   furnace       stove             3.11\n16       gem       jewel             3.84\n17     glass    magician             0.11\n18   journey         car             1.16\n19   journey      voyage             3.84\n20       lad     brother             1.66\n21       lad      wizard             0.42\n22  magician      wizard              3.5\n23    midday        noon             3.42\n24      monk      oracle              1.1\n25      monk       slave             0.55\n26      noon      string             0.08\n27   rooster      voyage             0.08\n28     shore    woodland             0.63\n29      tool   implement             1.68",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>word1</th>\n      <th>word2</th>\n      <th>human similarity</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>asylum</td>\n      <td>madhouse</td>\n      <td>3.61</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>bird</td>\n      <td>cock</td>\n      <td>3.05</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>bird</td>\n      <td>crane</td>\n      <td>2.97</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>boy</td>\n      <td>lad</td>\n      <td>3.76</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>brother</td>\n      <td>monk</td>\n      <td>2.82</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>car</td>\n      <td>automobile</td>\n      <td>3.92</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>cemetery</td>\n      <td>woodland</td>\n      <td>0.95</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>chord</td>\n      <td>smile</td>\n      <td>0.13</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>coast</td>\n      <td>forest</td>\n      <td>0.42</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>coast</td>\n      <td>hill</td>\n      <td>0.87</td>\n    </tr>\n    <tr>\n      <th>10</th>\n      <td>coast</td>\n      <td>shore</td>\n      <td>3.7</td>\n    </tr>\n    <tr>\n      <th>11</th>\n      <td>crane</td>\n      <td>implement</td>\n      <td>1.68</td>\n    </tr>\n    <tr>\n      <th>12</th>\n      <td>food</td>\n      <td>fruit</td>\n      <td>3.08</td>\n    </tr>\n    <tr>\n      <th>13</th>\n      <td>food</td>\n      <td>rooster</td>\n      <td>0.89</td>\n    </tr>\n    <tr>\n      <th>14</th>\n      <td>forest</td>\n      <td>graveyard</td>\n      <td>0.84</td>\n    </tr>\n    <tr>\n      <th>15</th>\n      <td>furnace</td>\n      <td>stove</td>\n      <td>3.11</td>\n    </tr>\n    <tr>\n      <th>16</th>\n      <td>gem</td>\n      <td>jewel</td>\n      <td>3.84</td>\n    </tr>\n    <tr>\n      <th>17</th>\n      <td>glass</td>\n      <td>magician</td>\n      <td>0.11</td>\n    </tr>\n    <tr>\n      <th>18</th>\n      <td>journey</td>\n      <td>car</td>\n      <td>1.16</td>\n    </tr>\n    <tr>\n      <th>19</th>\n      <td>journey</td>\n      <td>voyage</td>\n      <td>3.84</td>\n    </tr>\n    <tr>\n      <th>20</th>\n      <td>lad</td>\n      <td>brother</td>\n      <td>1.66</td>\n    </tr>\n    <tr>\n      <th>21</th>\n      <td>lad</td>\n      <td>wizard</td>\n      <td>0.42</td>\n    </tr>\n    <tr>\n      <th>22</th>\n      <td>magician</td>\n      <td>wizard</td>\n      <td>3.5</td>\n    </tr>\n    <tr>\n      <th>23</th>\n      <td>midday</td>\n      <td>noon</td>\n      <td>3.42</td>\n    </tr>\n    <tr>\n      <th>24</th>\n      <td>monk</td>\n      <td>oracle</td>\n      <td>1.1</td>\n    </tr>\n    <tr>\n      <th>25</th>\n      <td>monk</td>\n      <td>slave</td>\n      <td>0.55</td>\n    </tr>\n    <tr>\n      <th>26</th>\n      <td>noon</td>\n      <td>string</td>\n      <td>0.08</td>\n    </tr>\n    <tr>\n      <th>27</th>\n      <td>rooster</td>\n      <td>voyage</td>\n      <td>0.08</td>\n    </tr>\n    <tr>\n      <th>28</th>\n      <td>shore</td>\n      <td>woodland</td>\n      <td>0.63</td>\n    </tr>\n    <tr>\n      <th>29</th>\n      <td>tool</td>\n      <td>implement</td>\n      <td>1.68</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "output_type": "execute_result",
     "execution_count": 17
    }
   ],
   "source": [
    "import csv\n",
    "filename='mcdata.csv'\n",
    "\n",
    "with open(filename,'r') as filestream:\n",
    "    mcdata=list(csv.reader(filestream,delimiter=','))\n",
    "\n",
    "df=pd.DataFrame(mcdata,columns=[\"word1\",\"word2\",\"human similarity\"])\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the human similarity judgements range between 0 and 5.\n",
    "\n",
    "### Exercise 3.1\n",
    "Write code that will \n",
    "* compute the WordNet path_similarity for every pair of words in this data; and\n",
    "* add it as a column in the dataframe.  If you have the path similarity scores in a list called `scores`, you can do this using `df['path']=scores`\n",
    "\n",
    "Repeat for the Resnik and Lin similarity scores.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "       word1       word2 human similarity      path        res       lin\n0     asylum    madhouse             3.61  0.500000   9.475167  0.855584\n1       bird        cock             3.05  0.500000   7.677755  0.773937\n2       bird       crane             2.97  0.250000   7.677755  0.747812\n3        boy         lad             3.76  0.500000   8.399492  0.830562\n4    brother        monk             2.82  0.500000   9.261593  0.986407\n5        car  automobile             3.92  1.000000   7.591401  1.000000\n6   cemetery    woodland             0.95  0.111111   1.290026  0.123441\n7      chord       smile             0.13  0.090909   2.619644  0.246256\n8      coast      forest             0.42  0.166667   1.290026  0.130646\n9      coast        hill             0.87  0.200000   5.884681  0.599113\n10     coast       shore              3.7  0.500000   9.415744  0.963217\n11     crane   implement             1.68  0.200000   3.257679  0.359057\n12      food       fruit             3.08  0.100000   1.592755  0.160984\n13      food     rooster             0.89  0.062500   0.801759  0.091932\n14    forest   graveyard             0.84  0.111111   1.290026  0.123441\n15   furnace       stove             3.11  0.100000   2.305849  0.228138\n16       gem       jewel             3.84  1.000000  12.067705  1.000000\n17     glass    magician             0.11  0.125000   2.282647  0.214168\n18   journey         car             1.16  0.055556   0.000000  0.000000\n19   journey      voyage             3.84  0.500000   6.825958  0.777715\n20       lad     brother             1.66  0.200000   2.333545  0.255175\n21       lad      wizard             0.42  0.200000   2.333545  0.255091\n22  magician      wizard              3.5  1.000000  11.980693  1.000000\n23    midday        noon             3.42  1.000000  11.064403  1.000000\n24      monk      oracle              1.1  0.125000   2.333545  0.225652\n25      monk       slave             0.55  0.200000   2.333545  0.254311\n26      noon      string             0.08  0.083333   0.596229  0.066172\n27   rooster      voyage             0.08  0.041667   0.000000  0.000000\n28     shore    woodland             0.63  0.200000   1.290026  0.135583\n29      tool   implement             1.68  0.500000   5.877389  0.947154",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>word1</th>\n      <th>word2</th>\n      <th>human similarity</th>\n      <th>path</th>\n      <th>res</th>\n      <th>lin</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>asylum</td>\n      <td>madhouse</td>\n      <td>3.61</td>\n      <td>0.500000</td>\n      <td>9.475167</td>\n      <td>0.855584</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>bird</td>\n      <td>cock</td>\n      <td>3.05</td>\n      <td>0.500000</td>\n      <td>7.677755</td>\n      <td>0.773937</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>bird</td>\n      <td>crane</td>\n      <td>2.97</td>\n      <td>0.250000</td>\n      <td>7.677755</td>\n      <td>0.747812</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>boy</td>\n      <td>lad</td>\n      <td>3.76</td>\n      <td>0.500000</td>\n      <td>8.399492</td>\n      <td>0.830562</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>brother</td>\n      <td>monk</td>\n      <td>2.82</td>\n      <td>0.500000</td>\n      <td>9.261593</td>\n      <td>0.986407</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>car</td>\n      <td>automobile</td>\n      <td>3.92</td>\n      <td>1.000000</td>\n      <td>7.591401</td>\n      <td>1.000000</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>cemetery</td>\n      <td>woodland</td>\n      <td>0.95</td>\n      <td>0.111111</td>\n      <td>1.290026</td>\n      <td>0.123441</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>chord</td>\n      <td>smile</td>\n      <td>0.13</td>\n      <td>0.090909</td>\n      <td>2.619644</td>\n      <td>0.246256</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>coast</td>\n      <td>forest</td>\n      <td>0.42</td>\n      <td>0.166667</td>\n      <td>1.290026</td>\n      <td>0.130646</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>coast</td>\n      <td>hill</td>\n      <td>0.87</td>\n      <td>0.200000</td>\n      <td>5.884681</td>\n      <td>0.599113</td>\n    </tr>\n    <tr>\n      <th>10</th>\n      <td>coast</td>\n      <td>shore</td>\n      <td>3.7</td>\n      <td>0.500000</td>\n      <td>9.415744</td>\n      <td>0.963217</td>\n    </tr>\n    <tr>\n      <th>11</th>\n      <td>crane</td>\n      <td>implement</td>\n      <td>1.68</td>\n      <td>0.200000</td>\n      <td>3.257679</td>\n      <td>0.359057</td>\n    </tr>\n    <tr>\n      <th>12</th>\n      <td>food</td>\n      <td>fruit</td>\n      <td>3.08</td>\n      <td>0.100000</td>\n      <td>1.592755</td>\n      <td>0.160984</td>\n    </tr>\n    <tr>\n      <th>13</th>\n      <td>food</td>\n      <td>rooster</td>\n      <td>0.89</td>\n      <td>0.062500</td>\n      <td>0.801759</td>\n      <td>0.091932</td>\n    </tr>\n    <tr>\n      <th>14</th>\n      <td>forest</td>\n      <td>graveyard</td>\n      <td>0.84</td>\n      <td>0.111111</td>\n      <td>1.290026</td>\n      <td>0.123441</td>\n    </tr>\n    <tr>\n      <th>15</th>\n      <td>furnace</td>\n      <td>stove</td>\n      <td>3.11</td>\n      <td>0.100000</td>\n      <td>2.305849</td>\n      <td>0.228138</td>\n    </tr>\n    <tr>\n      <th>16</th>\n      <td>gem</td>\n      <td>jewel</td>\n      <td>3.84</td>\n      <td>1.000000</td>\n      <td>12.067705</td>\n      <td>1.000000</td>\n    </tr>\n    <tr>\n      <th>17</th>\n      <td>glass</td>\n      <td>magician</td>\n      <td>0.11</td>\n      <td>0.125000</td>\n      <td>2.282647</td>\n      <td>0.214168</td>\n    </tr>\n    <tr>\n      <th>18</th>\n      <td>journey</td>\n      <td>car</td>\n      <td>1.16</td>\n      <td>0.055556</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>19</th>\n      <td>journey</td>\n      <td>voyage</td>\n      <td>3.84</td>\n      <td>0.500000</td>\n      <td>6.825958</td>\n      <td>0.777715</td>\n    </tr>\n    <tr>\n      <th>20</th>\n      <td>lad</td>\n      <td>brother</td>\n      <td>1.66</td>\n      <td>0.200000</td>\n      <td>2.333545</td>\n      <td>0.255175</td>\n    </tr>\n    <tr>\n      <th>21</th>\n      <td>lad</td>\n      <td>wizard</td>\n      <td>0.42</td>\n      <td>0.200000</td>\n      <td>2.333545</td>\n      <td>0.255091</td>\n    </tr>\n    <tr>\n      <th>22</th>\n      <td>magician</td>\n      <td>wizard</td>\n      <td>3.5</td>\n      <td>1.000000</td>\n      <td>11.980693</td>\n      <td>1.000000</td>\n    </tr>\n    <tr>\n      <th>23</th>\n      <td>midday</td>\n      <td>noon</td>\n      <td>3.42</td>\n      <td>1.000000</td>\n      <td>11.064403</td>\n      <td>1.000000</td>\n    </tr>\n    <tr>\n      <th>24</th>\n      <td>monk</td>\n      <td>oracle</td>\n      <td>1.1</td>\n      <td>0.125000</td>\n      <td>2.333545</td>\n      <td>0.225652</td>\n    </tr>\n    <tr>\n      <th>25</th>\n      <td>monk</td>\n      <td>slave</td>\n      <td>0.55</td>\n      <td>0.200000</td>\n      <td>2.333545</td>\n      <td>0.254311</td>\n    </tr>\n    <tr>\n      <th>26</th>\n      <td>noon</td>\n      <td>string</td>\n      <td>0.08</td>\n      <td>0.083333</td>\n      <td>0.596229</td>\n      <td>0.066172</td>\n    </tr>\n    <tr>\n      <th>27</th>\n      <td>rooster</td>\n      <td>voyage</td>\n      <td>0.08</td>\n      <td>0.041667</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>28</th>\n      <td>shore</td>\n      <td>woodland</td>\n      <td>0.63</td>\n      <td>0.200000</td>\n      <td>1.290026</td>\n      <td>0.135583</td>\n    </tr>\n    <tr>\n      <th>29</th>\n      <td>tool</td>\n      <td>implement</td>\n      <td>1.68</td>\n      <td>0.500000</td>\n      <td>5.877389</td>\n      <td>0.947154</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "output_type": "execute_result",
     "execution_count": 18
    }
   ],
   "source": [
    "measures=[\"path\",\"res\",\"lin\"]\n",
    "for measure in measures:\n",
    "    scores=[]\n",
    "\n",
    "    for i,triple in enumerate(mcdata):\n",
    "        scores.append(word_similarity(triple[0],triple[1],measure=measure))\n",
    "    df[measure]=scores\n",
    "    \n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use pandas functionality to produce scatter plots and examine the correlation between different variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "x=\"human similarity\"\n",
    "y=\"path\"\n",
    "def draw_scatter(df , x , y):\n",
    "\n",
    "    df[x]=df[x].map(float)\n",
    "    df[y]=df[y].map(float)\n",
    "    \n",
    "    df.plot.scatter(x,y)\n",
    "draw_scatter(df,x,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3.2\n",
    "Generate scatter plots showing Resnik similarity against human similarity and Lin similarity against human similarity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "<Figure size 432x288 with 1 Axes>",
      "image/png": "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\n"
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "draw_scatter(df,x,\"res\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "<Figure size 432x288 with 1 Axes>",
      "image/png": "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\n"
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "draw_scatter(df,x,\"lin\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `DataFrame.corr()` method will compute the correlation for all pairs of columns with numeric values.  It is better to use Spearman's rank correlation coefficient than Pearson's product-moment correlation coefficient, since similarity scores are unlikely to be normally distributed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "                  human similarity      path       res       lin\nhuman similarity          1.000000  0.722743  0.735945  0.753510\npath                      0.722743  1.000000  0.900648  0.945509\nres                       0.735945  0.900648  1.000000  0.962707\nlin                       0.753510  0.945509  0.962707  1.000000",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>human similarity</th>\n      <th>path</th>\n      <th>res</th>\n      <th>lin</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>human similarity</th>\n      <td>1.000000</td>\n      <td>0.722743</td>\n      <td>0.735945</td>\n      <td>0.753510</td>\n    </tr>\n    <tr>\n      <th>path</th>\n      <td>0.722743</td>\n      <td>1.000000</td>\n      <td>0.900648</td>\n      <td>0.945509</td>\n    </tr>\n    <tr>\n      <th>res</th>\n      <td>0.735945</td>\n      <td>0.900648</td>\n      <td>1.000000</td>\n      <td>0.962707</td>\n    </tr>\n    <tr>\n      <th>lin</th>\n      <td>0.753510</td>\n      <td>0.945509</td>\n      <td>0.962707</td>\n      <td>1.000000</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "output_type": "execute_result",
     "execution_count": 22
    }
   ],
   "source": [
    "df.corr(method='spearman')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3.3\n",
    "* Looking at the scatter plots and the correlation coefficients, what do you conclude about the different WordNet similarity measures?\n",
    "* Do you have any reservations about your conclusions?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "source": [],
    "metadata": {
     "collapsed": false
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}