{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 5 (Part 2): Dictionary Methods for WSD\n",
    "\n",
    "We have seen that many words have many different senses.  In order to make the correct decision about the meaning of a sentence or a document, an application often needs to be able to **disambiguate** individual words, that is, choose the correct sense given the context.\n",
    "\n",
    "In this lab we will be looking st methods for word sense disambiguation (WSD) that make use of dictionaries or other lexical resources (also referred to as **knowledge-based methods** for WSD).  In particular, we will look at\n",
    "* simplified Lesk\n",
    "* adapted Lesk\n",
    "* minimising distance in a semantic hierarchy\n",
    "\n",
    "As in the previous lab, we will be using WordNet as our lexical resource.  So, first, lets import it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "Sussex NLTK root directory is \\\\ad.susx.ac.uk\\ITS\\TeachingResources\\Departments\\Informatics\\LanguageEngineering\\resources\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "from nltk import word_tokenize\n",
    "from nltk.corpus import wordnet as wn\n",
    "from nltk.corpus import wordnet_ic as wn_ic\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "\n",
    "import operator, sys\n",
    "from Week4Labs.utils import filter_stopwords, normalise\n",
    "\n",
    "sys.path.append(r'\\\\ad.susx.ac.uk\\ITS\\TeachingResources\\Departments\\Informatics\\LanguageEngineering\\resources')\n",
    "sys.path.append(r'/Users/juliewe/Documents/teaching/NLE2018/resources')\n",
    "from sussex_nltk.corpus_readers import AmazonReviewCorpusReader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "#make sure that the path to your utils.py file is correct for your computer\n",
    "sys.path.append('/Users/juliewe/Documents/teaching/NLE/NLE2019/w4/Week4Labs')\n",
    "\n",
    "from utils import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simplified Lesk\n",
    "\n",
    "The Lesk algorithm is based on the intuition that the correct combination of senses in a sentence will share more common words in their definitions.\n",
    "\n",
    "It is computationally very expensive to compare all possible sense combinations of words in a sentence.  If each word has just 2 senses, then there are $2^n$ possible sense combinations.\n",
    "\n",
    "In the simplifed Lesk algorithm, below, we consider each word in turn and choose the sense whose definition has more **overlap** with the contextual words in the sentence.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "def simplifiedLesk(word,sentence):\n",
    "    '''\n",
    "    Use the simplified Lesk algorithm to disambiguate word in the context of sentence\n",
    "    word: a String which is the word to be disambiguated\n",
    "    sentence: a String which is the sentence containing the word\n",
    "    :return: a pair (chosen sense definition, overlap score)\n",
    "    '''\n",
    "    \n",
    "    #construct the set of context word tokens for the sentence: all words in sentence - word itself\n",
    "    contexttokens=set(word_tokenize(sentence))-{word}\n",
    "    \n",
    "    #get all the possible synsets for the word\n",
    "    synsets=wn.synsets(word)\n",
    "    scores=[]\n",
    "    \n",
    "    #iterate over synsets\n",
    "    for synset in synsets:\n",
    "        #get the set of tokens in the definition of the synset\n",
    "        sensetokens=set(word_tokenize(synset.definition()))\n",
    "        #find the size of the intersection of the sensetokens set with the contexttokens set\n",
    "        scores.append((synset.definition(),len(sensetokens.intersection(contexttokens))))\n",
    "    \n",
    "    #sort the score list in descending order by the score (which is item with index 1 in the pair)\n",
    "    sortedscores=sorted(scores,key=operator.itemgetter(1),reverse=True) \n",
    "    #print(sortedscores)\n",
    "    return sortedscores[0]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "('the supreme effort one can make', {'the'})\n('the person who is most outstanding or excellent; someone who tops all others', {'is', 'the'})\n('Canadian physiologist (born in the United States) who assisted F. G. Banting in research leading to the discovery of insulin (1899-1978)', {'in', 'the'})\n('get the better of', {'the'})\n(\"(superlative of `good') having the most positive qualities\", {'the'})\n(\"(comparative and superlative of `well') wiser or more advantageous and hence advisable\", set())\n('having desirable or positive qualities especially those suitable for a thing specified', {'thing'})\n('having the normally expected amount', {'the'})\n('morally admirable', set())\n('deserving of esteem and respect', set())\n('promoting or enhancing well-being', set())\n('agreeable or pleasing', set())\n('of moral excellence', set())\n('having or showing knowledge and skill and aptitude', set())\n('thorough', set())\n('with or in a close or intimate relationship', {'in'})\n('financially sound', set())\n('most suitable or right for a particular purpose', set())\n('resulting favorably', set())\n('exerting force or influence', set())\n('capable of pleasing', set())\n('appealing to the mind', {'the'})\n('in excellent physical condition', {'in'})\n('tending to promote physical well-being; beneficial to health', set())\n('not forged', set())\n('not left to spoil', set())\n('generally admired', set())\n('in a most excellent way or manner', {'in'})\n('it would be sensible', set())\n('from a position of superiority or authority', set())\n(\"(often used as a combining form) in a good or proper or satisfactory manner or to a high standard (`good' is a nonstandard dialectal variant for `well')\", {'is', 'in'})\n('thoroughly or completely; fully; often used as a combining form; ; ; ; ; ,', set())\n('indicating high probability; in all likelihood', {'in'})\n('(used for emphasis or as an intensifier) entirely or fully', set())\n('to a suitable or appropriate extent or degree', set())\n('favorably; with approval', set())\n('to a great extent or degree', set())\n('with great or especially intimate knowledge', set())\n('with prudence or propriety', set())\n('with skill or in a pleasing manner', {'in'})\n('in a manner affording benefit or advantage', {'in'})\n('in financial comfort', {'in'})\n('without unusual distress or resentment; with good humor', set())\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "word=\"best\"\n",
    "contexttokens=set(word_tokenize(\"what is the best thing in this world\"))-{word}\n",
    "synsets=wn.synsets(word)\n",
    "scores=[]\n",
    "for synset in synsets:\n",
    "    #get the set of tokens in the definition of the synset\n",
    "    sensetokens=set(word_tokenize(synset.definition()))\n",
    "    #find the size of the intersection of the sensetokens set with the contexttokens set\n",
    "    scores.append((synset.definition(),sensetokens.intersection(contexttokens)))\n",
    "\n",
    "for i in scores:\n",
    "    print(i)\n",
    "    "
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now lets test it on a couple of sentences containing the word *bank*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "he borrowed money from the bank : ('a financial institution that accepts deposits and channels the money into lending activities', 2)\nhe sat on the bank of the river and watched the currents : ('sloping land (especially the slope beside a body of water)', 2)\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "banksentences=[\"he borrowed money from the bank\",\"he sat on the bank of the river and watched the currents\"]\n",
    "for sentence in banksentences:\n",
    "    print(sentence,\":\",simplifiedLesk(\"bank\",sentence))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It actually appears not to do too bad.  However, this is more by luck than anything else.   If you inspect the sentences and the definitions, you will notice that most of the overlap is currently generated by stopwords.\n",
    "\n",
    "### Exercise 1.1\n",
    "Improve the SimplifiedLesk algorithm by carrying out:\n",
    "* case and number normalisation \n",
    "* stopword filtering\n",
    "* lemmatisation\n",
    "\n",
    "You should find some useful functions for doing this in `utils.py` based on earlier labs.\n",
    "\n",
    "Make sure you test it.  Unfortunately, you should now find 0 overlap between any of the senses and the two bank sentences given."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "def simplifiedLesk(word,sentence):\n",
    "    '''\n",
    "    Use the simplified Lesk algorithm to disambiguate word in the context of sentence\n",
    "    word: a String which is the word to be disambiguated\n",
    "    sentence: a String which is the sentence containing the word\n",
    "    :return: a pair (chosen sense definition, overlap score)\n",
    "    '''\n",
    "    \n",
    "    #construct the set of context word tokens for the sentence: all words in sentence - word itself\n",
    "    lemma =WordNetLemmatizer()  \n",
    "    contexttokens=set((filter_stopwords(normalise(word_tokenize(sentence)))))-{word}\n",
    "    contextlemmas={lemma.lemmatize(contexttoken) for contexttoken in contexttokens}\n",
    "    \n",
    "    #get all the possible synsets for the word\n",
    "    synsets=wn.synsets(word)\n",
    "    scores=[]\n",
    "    \n",
    "    #iterate over synsets\n",
    "    for synset in synsets:\n",
    "        #get the set of tokens in the definition of the synset\n",
    "        sensetokens=set(filter_stopwords(normalise(word_tokenize(synset.definition()))))\n",
    "        senselemmas={lemma.lemmatize(token) for token in sensetokens}\n",
    "        #find the size of the intersection of the sensetokens set with the contexttokens set\n",
    "        scores.append((synset.definition(),len(senselemmas.intersection(contextlemmas))))\n",
    "    \n",
    "    #sort the score list in descending order by the score (which is item with index 1 in the pair)\n",
    "    sortedscores=sorted(scores,key=operator.itemgetter(1),reverse=True) \n",
    "    #print(sortedscores)\n",
    "    return sortedscores[0]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "he borrowed money from the bank : ('a financial institution that accepts deposits and channels the money into lending activities', 1)\nhe sat on the bank of the river and watched the currents : ('sloping land (especially the slope beside a body of water)', 0)\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "banksentences=[\"he borrowed money from the bank\",\"he sat on the bank of the river and watched the currents\"]\n",
    "for sentence in banksentences:\n",
    "    print(sentence,\":\",simplifiedLesk(\"bank\",sentence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "outputs": [
    {
     "data": {
      "text/plain": "[Synset('bank.n.01'),\n Synset('depository_financial_institution.n.01'),\n Synset('bank.n.03'),\n Synset('bank.n.04'),\n Synset('bank.n.05'),\n Synset('bank.n.06'),\n Synset('bank.n.07'),\n Synset('savings_bank.n.02'),\n Synset('bank.n.09'),\n Synset('bank.n.10'),\n Synset('bank.v.01'),\n Synset('bank.v.02'),\n Synset('bank.v.03'),\n Synset('bank.v.04'),\n Synset('bank.v.05'),\n Synset('deposit.v.02'),\n Synset('bank.v.07'),\n Synset('trust.v.01')]"
     },
     "metadata": {},
     "output_type": "execute_result",
     "execution_count": 28
    }
   ],
   "source": [
    "wn.synsets(\"bank\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adapted Lesk\n",
    "WordNet definitions are very short.  However, it is possible to create a bigger set of sense words by including information about the hypernyms and hyponyms of each sense.\n",
    "\n",
    "### Exercise 2.1\n",
    "Adapt the Lesk algorithm to include in `sensetokens`:\n",
    "* all of the lemma_names for the sense itself\n",
    "* all of the lemma_names for the hypernyms of the sense\n",
    "* all of the lemma_names for the hypoynyms of the sense\n",
    "* all of the words from the definitions of the hypernyms of the sense\n",
    "* all of the words from the definitions of the hyponyms of the sense\n",
    "\n",
    "Make sure you carry out normalisation and lemmatisation of these words as before\n",
    "\n",
    "Test each adaptation you make on the bank sentences, recording the overlap observed with the chosen sense."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "def adaptedLesk(word,sentence):\n",
    "    '''\n",
    "    Use the simplified Lesk algorithm to disambiguate word in the context of sentence, using standard WordNet adaptations\n",
    "    word: a String which is the word to be disambiguated\n",
    "    sentence: a String which is the sentence containing the word\n",
    "    :return: a pair (chosen sense definition, overlap score)\n",
    "    '''\n",
    "    \n",
    "    #construct the set of context word tokens for the sentence: all words in sentence - word itself\n",
    "    \n",
    "    lemma =WordNetLemmatizer()\n",
    "    contexttokens=set((filter_stopwords(normalise(word_tokenize(sentence)))))-{word}\n",
    "    contextlemmas={lemma.lemmatize(contexttoken) for contexttoken in contexttokens}\n",
    "    #get all the possible synsets for the word\n",
    "    synsets=wn.synsets(word)\n",
    "    scores=[]\n",
    "    \n",
    "    #iterate over synsets\n",
    "    for synset in synsets:\n",
    "        #get the set of tokens in the definition of the synset\n",
    "        sensetokens=word_tokenize(synset.definition())\n",
    "        sensetokens+=synset.lemma_names()\n",
    "        for hypernym in synset.hypernyms():\n",
    "            sensetokens+=hypernym.lemma_names()\n",
    "            sensetokens+=word_tokenize(hypernym.definition())\n",
    "        for hyponym in synset.hyponyms():\n",
    "            sensetokens+=hyponym.lemma_names()\n",
    "            sensetokens+=word_tokenize(hyponym.definition())\n",
    "        \n",
    "        sensetokens=set(filter_stopwords(normalise(sensetokens)))\n",
    "        senselemmas={lemma.lemmatize(token) for token in sensetokens}\n",
    "        #find the size of the intersection of the sensetokens set with the contexttokens set\n",
    "        scores.append((synset.definition(),len(senselemmas.intersection(contextlemmas))))\n",
    "    \n",
    "    #sort the score list in descending order by the score (which is item with index 1 in the pair)\n",
    "    sortedscores=sorted(scores,key=operator.itemgetter(1),reverse=True) \n",
    "    #print(sortedscores)\n",
    "    return sortedscores[0]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "he borrowed money from the bank : ('a financial institution that accepts deposits and channels the money into lending activities', 1)\nhe sat on the bank of the river and watched the currents : ('sloping land (especially the slope beside a body of water)', 1)\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "banksentences=[\"he borrowed money from the bank\",\"he sat on the bank of the river and watched the currents\"]\n",
    "for sentence in banksentences:\n",
    "    print(sentence,\":\",adaptedLesk(\"bank\",sentence))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2.2\n",
    "* From a sample of 1000 sentences from the dvd category of the Amazon review corpus (using the `sample_raw_sents()` method), find sentences which contain the lemma *film*. It will depend on the exact sample, but I would expect there to be somewhere between 50 and 100. \n",
    "* Use your AdaptedLesk algoritm to disambiguate them.  You may want to adapt it slightly so that it takes as input a list or a set of context lemmas rather than the sentence itself.  \n",
    "* Record the number of instances of each sense of *film* predicted by this algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "dvd_reader = AmazonReviewCorpusReader().category(\"dvd\")\n",
    "sentences=dvd_reader.sample_raw_sents(1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "The film is highly , highly recommended . : ('a form of entertainment that enacts a story by sound and a sequence of images giving the illusion of continuous movement', 0)\nThis movie is definetly worth a rental and is a surprising novelty watching a slasher film not featuring a bunch of half-clad bimbos running around but a bunch of half-clad himbos running around : ('a form of entertainment that enacts a story by sound and a sequence of images giving the illusion of continuous movement', 2)\nI 'd have to say this is one of the best animated films I 've ever seen . : ('a form of entertainment that enacts a story by sound and a sequence of images giving the illusion of continuous movement', 1)\nDespite some claims on here this film did fine at the box office it made $ 32,377,000 domestically . : ('a form of entertainment that enacts a story by sound and a sequence of images giving the illusion of continuous movement', 1)\nGiven that the film was shot in Panavision 's 2.35 : 1 anamorphic process , the scene would have played more effectively if Nolan had simply planted his camera for a medium shot of two skillful actors as they provided plot exposition . : ('photographic material consisting of a base of celluloid covered with a photographic emulsion; used to make negatives or transparencies', 3)\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "i=0\n",
    "for sentence in sentences:\n",
    "    if(i>=50 and i<=100):\n",
    "        if(\"film\" in sentence):\n",
    "            print(sentence,\":\",adaptedLesk(\"film\",sentence))\n",
    "    i+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2.3\n",
    "Inspect some of the individual predictions for your film sentences (at least one for each sense predicted).  Do you agree with the sense prediction?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "The music was specially composed for the film. : ('a form of entertainment that enacts a story by sound and a sequence of images giving the illusion of continuous movement', 0)\nShe develops her own film. : ('a form of entertainment that enacts a story by sound and a sequence of images giving the illusion of continuous movement', 0)\nThe crew has gone to Africa to film a wildlife documentary. : ('a form of entertainment that enacts a story by sound and a sequence of images giving the illusion of continuous movement', 1)\nHer last movie was filmed in Spain : ('a form of entertainment that enacts a story by sound and a sequence of images giving the illusion of continuous movement', 1)\nI didn’t get my film developed yet. : ('a form of entertainment that enacts a story by sound and a sequence of images giving the illusion of continuous movement', 0)\na roll of film : ('photographic material consisting of a base of celluloid covered with a photographic emulsion; used to make negatives or transparencies', 1)\nA film of oil glistened on the surface of the water. : ('a thin coating or layer', 3)\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "film_sentences=[\"The music was specially composed for the film.\",\"She develops her own film.\",\"The crew has gone to Africa to film a wildlife documentary.\",\"Her last movie was filmed in Spain\",\"I didn’t get my film developed yet.\",\"a roll of film\",\"A film of oil glistened on the surface of the water.\"]\n",
    "for film_sentence in film_sentences:\n",
    "    print(film_sentence,\":\",adaptedLesk(\"film\",film_sentence))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Minimising the Distance in the Semantic Hierarchy\n",
    "This WSD method is based on the intuition that the concepts mentioned in a sentence will be close together in the hyponym hierarchy.\n",
    "\n",
    "### Exercise 3.1\n",
    "Write a function `max_sim(word, contextlemmas,pos)`which will choose the sense of a *word* given its context *sentence* using a WordNet based semantic similarity measure (see Lab_5_1).  You can assume that the part of speech of the word is known and is supplied to the function as another argument.\n",
    "\n",
    "Within the function, \n",
    "1. For each **sense** of the word under consideration:\n",
    "* compute its semantic similarity with each context **lemma** of the same part of speech.  For each context lemma you will need to consider each of its **senses** (and take the maximum similarity).  Therefore, you will need a triple nested loop! \n",
    "* sum the semantic similarities over the sentence\n",
    "2. Choose the **sense** with the maximum sum.\n",
    "\n",
    "Test your function on the bank sentences.  You should find, disappointingly for the method,  that the first sentence has a maximum score of 2.71 with \"an arrangement of similar objects in a row or in tiers\" and the second sentence has a maximum socre of 4.68 with \"an arrangement of similar objects in a row or in tiers\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "def max_sim(word,contextlemmas,pos=wn.NOUN):\n",
    "    #brown_ic=wn_ic.ic(\"ic-brown.dat\")\n",
    "    synsets=wn.synsets(word,pos)\n",
    "    scores=[]\n",
    "    for synset in synsets:\n",
    "        total=0\n",
    "        for lemma in contextlemmas:\n",
    "            sofar=0\n",
    "            for synsetB in wn.synsets(lemma,pos):\n",
    "                sim=wn.path_similarity(synset,synsetB)\n",
    "                if sim>sofar:\n",
    "                    sofar=sim\n",
    "            total+=sofar\n",
    "        scores.append((synset.definition(),total))\n",
    "    sortedscores=sorted(scores,key=operator.itemgetter(1),reverse=True)\n",
    "    #print(sortedscores)\n",
    "    return sortedscores[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "he borrowed money from the bank : ('a supply or stock held in reserve for future use (especially in emergencies)', 1.482901085763742)\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "banksentences=[\"he borrowed money from the bank\",\"he sat on the bank of the river and watched the currents\"]\n",
    "\n",
    "sentence=filter_stopwords(normalise(word_tokenize(banksentences[0])))\n",
    "print(banksentences[0],\":\",max_sim(\"bank\",sentence))\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3.2\n",
    "* Run your max_sim function on all of your film sentences and record the number of predictions for each sense.\n",
    "* Inspect some of the individual predictions.\n",
    "* Compare the results with those from the AdaptedLesk algorithm and draw some conclusions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-54f22deb0ea3>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mdvd_reader\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mAmazonReviewCorpusReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcategory\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"kitchen\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0msentences\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdvd_reader\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msample_raw_sents\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1000\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0msentence\u001b[0m \u001b[1;32min\u001b[0m \u001b[0msentences\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[1;32mif\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m>=\u001b[0m\u001b[1;36m50\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m<=\u001b[0m\u001b[1;36m100\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'AmazonReviewCorpusReader' is not defined"
     ],
     "ename": "NameError",
     "evalue": "name 'AmazonReviewCorpusReader' is not defined",
     "output_type": "error"
    }
   ],
   "source": [
    "dvd_reader = AmazonReviewCorpusReader().category(\"kitchen\")\n",
    "sentences=dvd_reader.sample_raw_sents(1000)\n",
    "i=0\n",
    "for sentence in sentences:\n",
    "    if(i>=50 and i<=100):\n",
    "        if(\"film\" in sentence):\n",
    "            print(\"adaptLesk:\",adaptedLesk(\"\",sentence))\n",
    "            print(\"lin_sim:\",max_sim(\"film\",filter_stopwords(normalise(word_tokenize(sentence)))))\n",
    "            print(sentence)\n",
    "    i+=1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "adaptLesk: ('a form of entertainment that enacts a story by sound and a sequence of images giving the illusion of continuous movement', 0)\n",
      "lin_sim: ('a form of entertainment that enacts a story by sound and a sequence of images giving the illusion of continuous movement', 1.2523973849914365)\nThe music was specially composed for the film.\nadaptLesk: ('a form of entertainment that enacts a story by sound and a sequence of images giving the illusion of continuous movement', 0)\n",
      "lin_sim: ('a form of entertainment that enacts a story by sound and a sequence of images giving the illusion of continuous movement', 1.0)\nShe develops her own film.\nadaptLesk: ('a form of entertainment that enacts a story by sound and a sequence of images giving the illusion of continuous movement', 1)\n",
      "lin_sim: ('a form of entertainment that enacts a story by sound and a sequence of images giving the illusion of continuous movement', 2.135407478380542)\nThe crew has gone to Africa to film a wildlife documentary.\nadaptLesk: ('a form of entertainment that enacts a story by sound and a sequence of images giving the illusion of continuous movement', 1)\n",
      "lin_sim: ('a form of entertainment that enacts a story by sound and a sequence of images giving the illusion of continuous movement', 1.4170901816405477)\nHer last movie was filmed in Spain\nadaptLesk: ('a form of entertainment that enacts a story by sound and a sequence of images giving the illusion of continuous movement', 0)\n",
      "lin_sim: ('a form of entertainment that enacts a story by sound and a sequence of images giving the illusion of continuous movement', 1.2547193319791856)\nI didn’t get my film developed yet.\nadaptLesk: ('photographic material consisting of a base of celluloid covered with a photographic emulsion; used to make negatives or transparencies', 1)\n",
      "lin_sim: ('photographic material consisting of a base of celluloid covered with a photographic emulsion; used to make negatives or transparencies', 1.8823609641633539)\na roll of film\nadaptLesk: ('a thin coating or layer', 3)\n",
      "lin_sim: ('a thin sheet of (usually plastic and usually transparent) material used to wrap or cover things', 2.080707008228772)\nA film of oil glistened on the surface of the water.\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "film_sentences=[\"The music was specially composed for the film.\",\"She develops her own film.\",\"The crew has gone to Africa to film a wildlife documentary.\",\"Her last movie was filmed in Spain\",\"I didn’t get my film developed yet.\",\"a roll of film\",\"A film of oil glistened on the surface of the water.\"]\n",
    "for film_sentence in film_sentences:\n",
    "    print(\"adaptLesk:\",adaptedLesk(\"film\",film_sentence))\n",
    "    print(\"lin_sim:\",max_sim(\"film\",filter_stopwords(normalise(word_tokenize(film_sentence)))))\n",
    "    print(film_sentence)\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "data": {
      "text/plain": "[Synset('movie.n.01'),\n Synset('film.n.02'),\n Synset('film.n.03'),\n Synset('film.n.04'),\n Synset('film.n.05'),\n Synset('film.v.01'),\n Synset('film.v.02')]"
     },
     "metadata": {},
     "output_type": "execute_result",
     "execution_count": 5
    }
   ],
   "source": [
    "wn.synsets(\"film\")\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "source": [],
    "metadata": {
     "collapsed": false
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}