{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 8: Part-of-Speech Tagging \n",
    "\n",
    "This week we are learn about part-of-speech (POS) tagging.  This involves deciding the correct part-of speech tag (e.g., noun, verb, adjective etc) for each word in a sentence.  Since the correct tag for each word depends not only on the current word but on the tags of those words around it, it is generally viewed as a **sequence labelling** problem.  In other words, for a given sequence of words, we are asking what is the most likely sequence of tags?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(r'\\\\ad.susx.ac.uk\\ITS\\TeachingResources\\Departments\\Informatics\\LanguageEngineering\\resources')\n",
    "sys.path.append(r'/Users/juliewe/resources')\n",
    "\n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from itertools import zip_longest\n",
    "%matplotlib inline\n",
    "import random\n",
    "import math\n",
    "import operator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Average PoS tag ambiguity \n",
    "The Part-of-Speech (PoS) tag ambiguity of a word type is a measure of how varied the PoS tags are for that type.   Note that here, we talk about the ambiguity of a word type rather than a word token because any given token has a single tag but different occurrences of the same type may have different tags.  For example, some occurrences of the word *bank* have the tag *noun* whereas others have the tag *verb*\n",
    "\n",
    "Some types are always (or almost always) labelled with the same PoS tag, so exhibit no (or very little) ambiguity. It is easy to predict the correct PoS tag for such words. \n",
    "\n",
    "On the other hand, a type that is commonly labelled by a variety of different PoS tags exhibits a high level of ambiguity, and is more challenging to deal with.\n",
    "\n",
    "In this session, we are going to be considering two measures of a type's ambiguity. We will be using the Wall Street Journal corpus as it has been hand-annotated with part of speech tags. \n",
    "We will consider \n",
    "* a simple measure that just **counts** the number of different tags that label the type. \n",
    "* a more complex information-theoretic measure based on **entropy**.\n",
    "\n",
    "First, we create an instance of a `WSJCorpusReader`.  Then we can use the method `tagged_words()` to get a list of all tokens in the corpus tagged with their POS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sussex_nltk.corpus_readers import WSJCorpusReader\n",
    "\n",
    "wsjreader=WSJCorpusReader()\n",
    "taggedWSJ=wsjreader.tagged_words()\n",
    "for i,(token,tag) in enumerate(taggedWSJ):\n",
    "    print(i,token,tag)\n",
    "    if i>10:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1.1\n",
    "Write a function `find_tag_distributions(tokentaglist)` which finds the (frequency) distributions of tags for every word in its input.\n",
    "* input: a list of pairs (token,tag)\n",
    "* returns: a dictionary of dictionaries.  The key to the outermost dictionary should be the word and the key to each internal dictionary should be the tag.  The value associated with the tag in the internal dictionary should be its frequency of occurrence.\n",
    "\n",
    "Note that this exercise is very similar to Ex1.1 in Lab_7_1\n",
    "\n",
    "Test your function on `taggedWSJ` and look at the tag distribution for the word `bank`.  You should find that you get:\n",
    "\n",
    "`{NN: 521,\n",
    "VB: 1,\n",
    "VBP: 1}`\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1.2\n",
    "Write a function `simple_pos_ambiguity` which can take the tagged WSJ text and returns a dictionary containing the number of part of speech tags which each word type has.  Note that this is simply the length of the dictionary associated with that word in the output from `find_tag_distributions`.\n",
    "\n",
    "Check that you get the following results:\n",
    "bank: 3\n",
    "blue: 2\n",
    "walk: 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1.3\n",
    "Find the mean average value of the `simple_pos_ambiguity` score for word types in the WSJ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Entropy as a Measure of Tag Ambiguity\n",
    "\n",
    "**Entropy** is a measure of uncertainty. A word will have high entropy when it occurs the same number of times with each part of speech. There is maximum uncertainty as to which part of speech it has.\n",
    "\n",
    "The larger the part of speech tagset, the greater the potential for uncertainty, and the higher the entropy can be.\n",
    "\n",
    "In the cell below we see a function `entropy`. It's argument is a list of counts (which in our case are counts of how many times a word appeared with a given part of speech).\n",
    "\n",
    "Check that you understand how the code implements this definition of entropy:\n",
    "$$H([x_1,\\ldots,x_n])= - \\sum_{i=1}^nP(x_i)\\log_2 P(x_i)$$\n",
    "where $n$ is the number of PoS tags, and $x_i$ is a count of how many times the word was labelled with the $i$th PoS tag."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def entropy(counts):            # counts = list of counts of occurrences of tags\n",
    "    total = sum(counts)         # get total number of occurrences\n",
    "    if not total: return 0      # if zero occurrences in total, then 0 entropy\n",
    "    entropy = 0\n",
    "    for i in counts:            # for each tag count\n",
    "        p = i/total      # probability that the token occurs with this tag\n",
    "        try:\n",
    "            entropy += p * math.log(p,2) # add to entropy\n",
    "        except ValueError: pass     # if p==0, then ignore this p\n",
    "    return -entropy if entropy else entropy   # only negate if nonzero, otherwise \n",
    "                                              # floats can return -0.0, which is weird.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2.1\n",
    "Experiment with the `entropy` function.\n",
    "- It takes a list of counts as its argument.\n",
    "- Compare the entropy of a list where all counts are the same with the entropy of a list of different counts.\n",
    "- See what happens when you vary the length of the list of counts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2.2\n",
    "Write a function `entropy_ambiguity` which takes the tagged WSJ text and returns a dictionary containing the entropy of each word.\n",
    "\n",
    "Test it out your function; you should find:\n",
    "\n",
    "`bank: 0.04004053596567404\n",
    "blue: 0.4394969869215134\n",
    "walk: 1.3127443531093745\n",
    "show: 1.5322594002899546`\n",
    "\n",
    "How does this correspond to our intuitions about which word types are more difficult to correctly POS tag?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A Simple Unigram Tagger\n",
    "Now, we will be looking at part of speech tagging itself i.e., the problem of determining the correct tag for a given word token. We will\n",
    "\n",
    "* implement a unigram tagger\n",
    "* experiment with an off-the-shelf POS tagger which utilises information about the previous words or tags in the sequence.\n",
    "\n",
    "First, lets get some tagged text from the WSJ and split it into a training and a testing set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_train_test_pos(split=0.7):\n",
    "\n",
    "    from sussex_nltk.corpus_readers import WSJCorpusReader\n",
    "    wsjreader=WSJCorpusReader()\n",
    "    taggedWSJ=wsjreader.tagged_words()\n",
    "    taggedlist=list(taggedWSJ)\n",
    "    \n",
    "    #we don't want to randomly select data because we need to preserve sequence information\n",
    "    #so we are just going to take the first part as training and the second as test\n",
    "    n=int(len(taggedlist)*split)\n",
    "    return taggedlist[:n],taggedlist[n:]\n",
    "\n",
    "train, test = get_train_test_pos(split=0.8)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we build a unigram model of the tag distribution for each word type.  We use the `find_tag_distributions` function defined earlier and store the result in the variable `unigram_model`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unigram_model=find_tag_distributions(train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3.1\n",
    "Write a `uni_pos_tag` function which takes:\n",
    "* a sequence of tokens \\[wordtoken1,wordtoken2, ....\\]\n",
    "* a unigram model (stored as a dictionary of dictionaries\n",
    "and returns:\n",
    "* a tagged sequence of tokens \\[(wordtoken1,tag1),(wordtoken2,tag2),....\\]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3.2\n",
    "Test that your function works on both the training data `train` and the testing data `test`.  Remember, you can separate the tokens and the tags into two separate lists using:\n",
    "* `train_toks,train_tags=zip(*train)`\n",
    "* `test_toks,test_tags=zip(*test)`\n",
    "\n",
    "Don't worry about evaluating the accuracy at this point (that's the next exercise) - just check that you can generate sequences of (token,tag) pairs in both cases.  What happens if there is a word in the test data that didn't occur in the training data?  You might need to update your `uni_pos_tag` function to take this into account."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3.3\n",
    "Write a function `evaluate_uni_pos_tag` which will calculate the accuracy of the `uni_pos_tag` function. This should have as arguments:\n",
    "* the unigram_model\n",
    "* the gold standard sequence of (token,tag) pairs for comparison\n",
    "\n",
    "You should find that it is 94.6% accurate on the training data.  How accurate is it on the test data? \n",
    "\n",
    "As an extension, you could implement a uni_pos_tagger class, which combines the all of the functionality above, and then provide an `evaluate` function which evaluates a tagger. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Beyond Unigram Tagging\n",
    "State-of-the-art POS-taggers use information about likely sequences of tags to get higher performance.\n",
    "\n",
    "The `pos_tag` function provided by nltk uses a pre-trained maximum entropy markov model (MEMM).  We can run it on our sequences of tokens in the same way as our `uni_pos_tag` function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import pos_tag\n",
    "\n",
    "pos_tag(train_toks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 4.1\n",
    "Write or adapt your code so that you can evaluate `nltk pos_tag` function on the training and testing data, as divided above.  What improvement over unigram tagging does the nltk pos tagger provide?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extension\n",
    "Find examples where the unigram tagger makes mistakes but the nltk pos tagger is correct.  What different types of errors are being made?  Can you explain intuitively why the correct sequence predicted by the nltk pos tagger is more likely than the one predicted by the unigram tagger?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
