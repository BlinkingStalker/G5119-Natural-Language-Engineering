{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 6 (Part 2): Sentiment Lexicons\n",
    "In this lab we will be looking at lexicons for sentiment analysis.  In particular, we will be investigating:\n",
    "* bootstrapping wordlists using WordNet\n",
    "* bootstrapping wordlists from corpora\n",
    "\n",
    "First some preliminary imports:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import wordnet as wn\n",
    "import nltk\n",
    "import operator\n",
    "import random\n",
    "import sys\n",
    "import pandas as pd\n",
    "from Week6Labs.classifiercode import feature_extract, get_training_test_data, SimpleClassifier, classifier_evaluate\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to use a WordList classifier (developed in Wk3) and evaluation code (developed in Wk4). So lets import it here "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "Sussex NLTK root directory is \\\\ad.susx.ac.uk\\ITS\\TeachingResources\\Departments\\Informatics\\LanguageEngineering\\resources\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "from classifiercode import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now lets define some short lists of positive and negative words, that we might expect to find in movie reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "my_positive_words=[\"awesome\",\"thrilling\",\"funny\",\"great\"]\n",
    "my_negative_words=[\"boring\",\"terrible\",\"hate\",\"waste\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Grab some training and testing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "training,testing=get_training_test_data(\"dvd\")\n",
    "traindata=[(feature_extract(review),label) for (review,label) in training]\n",
    "testdata=[(feature_extract(review),label) for (review,label) in testing]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Construct a simple wordlist classifier (no training).  This is going to give us our baseline performance which we are going to try to beat."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "baseline=SimpleClassifier(my_positive_words,my_negative_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now lets evaluate it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "[0.55, 0.5270758122743683, 0.9733333333333334, 0.6838407494145199]"
     },
     "metadata": {},
     "output_type": "execute_result",
     "execution_count": 8
    }
   ],
   "source": [
    "ms=[\"accuracy\",\"precision\",\"recall\",\"f1\"]\n",
    "classifier_evaluate(baseline,testing,ms)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What do these figures tell you about the baseline classifier?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Bootstrapping Wordlists from WordNet\n",
    "\n",
    "We are going to use semantic relationships in WordNet to extend the wordlists we have created.  Using human input to *seed* an algorithm in this way is often referred to as **bootstrapping**.  It often also gets referred to as **semi-supervised** learning. \n",
    "\n",
    "A useful helper function `flatten` is defined below.  This takes an arbitrarily nested list and flattens it into a one level list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flatten(nested_list):\n",
    "    \"\"\"\n",
    "    flatten an arbitrarily nested list\n",
    "    :param nested_list: list structure potentially containing more lists\n",
    "    :return: list of atomic items\n",
    "    \"\"\"\n",
    "    if isinstance(nested_list,str):\n",
    "        return [nested_list]\n",
    "    elif isinstance(nested_list,list):\n",
    "        res=[]\n",
    "        for item in nested_list:\n",
    "            res+=flatten(item)\n",
    "        return res\n",
    "    else:\n",
    "        return [nested_list]\n",
    "\n",
    "\n",
    "flatten([[[1,2],4],[5,6]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1.1\n",
    "Write a function `find_words(word,relation)` which takes two arguments, a word and a relation, and returns the **set** of all of the words which are in the given relation with the given word according to WordNet.  For example:\n",
    "* find_words(\"car\",\"synonym\") should return  {'auto','automobile','cable_car','car','elevator_car','gondola','machine','motorcar','railcar','railroad_car','railway_car'}\n",
    "* find_words(\"car\",\"hyponym\") should return a set of 83 words\n",
    "* find_words(\"car\",\"hypernym\") should return a set of 4 words\n",
    "* find_words(\"car\",\"antonym\") should return an empty set\n",
    "\n",
    "Hint: one way of doing this is to use nested list comprehensions, flatten the resulting list using the `flatten()` function defined above and then use the built-in `set()` function to remove duplicates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1.2\n",
    "Use your `find_words()` function to extend your lists of seeds in 4 different ways:-\n",
    "1. add all synonyms\n",
    "2. add all synonyms and antonyms\n",
    "3. add all synonyms, antonyms and hyponyms\n",
    "4. add all synonyms, antonyms, hyponyms and hypernyms\n",
    "\n",
    "In each case, think about whether the related words should be added to the **same** seed list or to the **other** seed list.\n",
    "\n",
    "Starting with the seed lists defined above, the lengths of your extended lists should be\n",
    "1. positive: 68, negative: 73\n",
    "2. positive: 71, negative: 73 \n",
    "3. positive: 71, negative: 168\n",
    "4. positive: 91, negative: 219\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1.3\n",
    "* Build classifiers using each of the four variations of wordlist extensions. \n",
    "* Test them on the testing set\n",
    "* Display your results (including the baseline) in a pandas table\n",
    "* Make a barchart showing the accuracy scores for the different variations.\n",
    "* Interpret your results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Finding Word Patterns in Corpus Data\n",
    "\n",
    "Now we are going to use the training data as a corpus to search for words which tend to be **conjoined** with the seed words.  For example, \"funny and fresh\" would indicate that \"fresh\" is also a positive sentiment word.  On the other hand, \"funny but predictable\" indicates that \"predictable\" is a negative sentiment word. \n",
    "\n",
    "Note that whilst we are using the labelled training data, we will not be paying attention to the positive and negative labels.  Therefore this is essentially an unsupervised method (we could use any in-domain corpus data even if it had not be labelled with sentiment).\n",
    "\n",
    "First, lets flatten the corpus into a list of tokens and normalise the corpus (stop-word removal and lower-casing).  We are not going to use the bag-of-words representation here because we want to pay attention to the order of the words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_corpus=flatten([review.sents() for review,label in training])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalised_corpus=[normalise(token) for token in training_corpus]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalised_corpus[:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2.1\n",
    "\n",
    "Write a function `search_conj()` to find words which are co-ordinated.  Your function should take 3 arguments:\n",
    "* the conjunction word e.g., one of {\"and\", \"or\", \"but\"}\n",
    "* the seed word\n",
    "* the corpus\n",
    "\n",
    "It should return a set of words.  You should filter this set for (at least) stopwords and punctuation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2.2\n",
    "Update your `find_words()` and `extend()` functions as necessary so that you can use them to add words found by `search_conj()` to seed word lists.  Then, extend your **original** seed word lists by \n",
    "1. adding words conjoined with *and*\n",
    "2. adding words conjoined with *and* and with *but*\n",
    "\n",
    "When I ran this, I found that I have lists of the following lengths:\n",
    "1. positive: 28, negative: 17\n",
    "2. positive: 28, negative: 25\n",
    "\n",
    "However, your lists will be of a different length due to the random nature of the sample of sentences selected from the corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2.3\n",
    "Build and evaluate classifiers using these wordlists.  Make sure you provide a visualation of  your results (e.g., Pandas barchart) and draw some conclusions.  What are the advantages and disadvantages of each method (using WordNet vs using corpora) to extend the wordlists?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "source": [],
    "metadata": {
     "collapsed": false
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}