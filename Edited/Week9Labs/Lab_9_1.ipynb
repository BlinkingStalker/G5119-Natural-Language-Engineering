{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 9: Named Entity Recognition\n",
    "\n",
    "This week we are going to looking at named entity recognition in the fiction genre. In doing so we will introduce the spaCy library (https://spacy.io/) which provides a number of very fast, state-of-the-art accuracy tools for carrying out NLP tasks including part-of-speech tagging, dependency parsing and named entity recognition.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#preliminary imports\n",
    "import sys\n",
    "#sys.path.append(r'T:\\Departments\\Informatics\\LanguageEngineering') \n",
    "sys.path.append(r'/Users/juliewe/resources')\n",
    "sys.path.append(r'C:\\Users\\Alex\\resources')\n",
    "sys.path.append(r'/Users/Alex/resources')\n",
    "\n",
    "import pandas as pd\n",
    "import operator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Project Gutenberg\n",
    "\n",
    "[Project Gutenberg electronic text archive](http://www.gutenberg.org/) contains around 25,000 free electronic books.\n",
    "\n",
    "A small selection is made available through the NLTK. For the full list, run the following cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "data": {
      "text/plain": "['austen-emma.txt',\n 'austen-persuasion.txt',\n 'austen-sense.txt',\n 'bible-kjv.txt',\n 'blake-poems.txt',\n 'bryant-stories.txt',\n 'burgess-busterbrown.txt',\n 'carroll-alice.txt',\n 'chesterton-ball.txt',\n 'chesterton-brown.txt',\n 'chesterton-thursday.txt',\n 'edgeworth-parents.txt',\n 'melville-moby_dick.txt',\n 'milton-paradise.txt',\n 'shakespeare-caesar.txt',\n 'shakespeare-hamlet.txt',\n 'shakespeare-macbeth.txt',\n 'whitman-leaves.txt']"
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import gutenberg\n",
    "gutenberg.fileids()\n",
    "nltk.corpus.gutenberg.fileids()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can get the raw text of any of the novels using the `gutenberg.raw(fileid)` method.  This returns a String."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "\"[Emma by Jane Austen 1816]\\n\\nVOLUME I\\n\\nCHAPTER I\\n\\n\\nEmma Woodhouse, handsome, clever, and rich, with a comfortable home\\nand happy disposition, seemed to unite some of the best blessings\\nof existence; and had lived nearly twenty-one years in the world\\nwith very little to distress or vex her.\\n\\nShe was the youngest of the two daughters of a most affectionate,\\nindulgent father; and had, in consequence of her sister's marriage,\\nbeen mistress of his house from a very early period.  Her mother\\nhad died t\""
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emma=gutenberg.raw('austen-emma.txt')\n",
    "emma[:500]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we carry out a little bit of cleaning of the text.  Check you understand what each line in the `clean_text()` function does."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "887071\n",
      "880067\n"
     ]
    },
    {
     "data": {
      "text/plain": "\"Emma Woodhouse, handsome, clever, and rich, with a comfortable home and happy disposition, seemed to unite some of the best blessings of existence; and had lived nearly twenty-one years in the world with very little to distress or vex her. She was the youngest of the two daughters of a most affectionate, indulgent father; and had, in consequence of her sister's marriage, been mistress of his house from a very early period. Her mother had died too long ago for her to have more than an indistinct \""
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "def clean_text(astring):\n",
    "    #replace newlines with space\n",
    "    newstring=re.sub(\"\\n\",\" \",astring)\n",
    "    #remove title and chapter headings\n",
    "    newstring=re.sub(\"\\[[^\\]]*\\]\",\" \",newstring)\n",
    "    newstring=re.sub(\"VOLUME \\S+\",\" \",newstring)\n",
    "    newstring=re.sub(\"CHAPTER \\S+\",\" \",newstring)\n",
    "    newstring=re.sub(\"\\s\\s+\",\" \",newstring)\n",
    "    return newstring.lstrip().rstrip()\n",
    "\n",
    "clean_emma=clean_text(emma)\n",
    "print(len(emma))\n",
    "print(len(clean_emma))\n",
    "clean_emma[:500]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SpaCy\n",
    "\n",
    "If working at home, you may need to install spaCy and download a set of English models.  at the command line:\n",
    "\n",
    "` pip install spacy\n",
    "python -m spacy download en_core_web_sm`\n",
    "\n",
    "In the lab, or once you have done this at home, you should then be able to set up a spaCy processing pipeline as follows:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "data": {
      "text/plain": "spacy.lang.en.English"
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import spacy\n",
    "#nlp=spacy.load('en')\n",
    "\n",
    "nlp=spacy.load('en_core_web_sm')\n",
    "type(nlp)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "import en_core_web_sm\n",
    "nlp = en_core_web_sm.load()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can run any text string through the language processing pipeline stored in `nlp`\n",
    "This next cell might take a few minutes to run since it carries out all of the SpaCy NLP functionality on the input text.  It will return a spaCy `Doc` object which contains the text plus various annotations.  See the SpaCy documentation https://spacy.io/api/doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp_emma=nlp(clean_emma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "spacy.tokens.doc.Doc"
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(nlp_emma)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For example, we can now iterator over sentences in the document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Emma Woodhouse, handsome, clever, and rich, with a comfortable home and happy disposition, seemed to unite some of the best blessings of existence; and had lived nearly twenty-one years in the world with very little to distress or vex her.\n"
     ]
    }
   ],
   "source": [
    "for s in nlp_emma.sents:\n",
    "    print(s)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can iterate over tokens in sentences and find out the labels added by SpaCy to each token.  Look at the SpaCy documentation https://spacy.io/api/token for more information about the Token object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "emma_sents=list(nlp_emma.sents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "          token        lower        lemma    pos     NER\n0          Emma         emma         Emma  PROPN  PERSON\n1     Woodhouse    woodhouse    Woodhouse  PROPN  PERSON\n2             ,            ,            ,  PUNCT        \n3      handsome     handsome     handsome    ADJ        \n4             ,            ,            ,  PUNCT        \n5        clever       clever       clever    ADJ        \n6             ,            ,            ,  PUNCT        \n7           and          and          and  CCONJ        \n8          rich         rich         rich    ADJ        \n9             ,            ,            ,  PUNCT        \n10         with         with         with    ADP        \n11            a            a            a    DET        \n12  comfortable  comfortable  comfortable    ADJ        \n13         home         home         home   NOUN        \n14          and          and          and  CCONJ        \n15        happy        happy        happy    ADJ        \n16  disposition  disposition  disposition   NOUN        \n17            ,            ,            ,  PUNCT        \n18       seemed       seemed         seem   VERB        \n19           to           to           to   PART        \n20        unite        unite        unite   VERB        \n21         some         some         some    DET        \n22           of           of           of    ADP        \n23          the          the          the    DET        \n24         best         best         good    ADJ        \n25    blessings    blessings     blessing   NOUN        \n26           of           of           of    ADP        \n27    existence    existence    existence   NOUN        \n28            ;            ;            ;  PUNCT        \n29          and          and          and  CCONJ        \n30          had          had         have    AUX        \n31        lived        lived         live   VERB        \n32       nearly       nearly       nearly    ADV    DATE\n33       twenty       twenty       twenty    NUM    DATE\n34            -            -            -  PUNCT    DATE\n35          one          one          one    NUM    DATE\n36        years        years         year   NOUN    DATE\n37           in           in           in    ADP        \n38          the          the          the    DET        \n39        world        world        world   NOUN        \n40         with         with         with    ADP        \n41         very         very         very    ADV        \n42       little       little       little    ADJ        \n43           to           to           to   PART        \n44     distress     distress     distress   VERB        \n45           or           or           or  CCONJ        \n46          vex          vex          vex   VERB        \n47          her          her       -PRON-   PRON        \n48            .            .            .  PUNCT        ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>token</th>\n      <th>lower</th>\n      <th>lemma</th>\n      <th>pos</th>\n      <th>NER</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Emma</td>\n      <td>emma</td>\n      <td>Emma</td>\n      <td>PROPN</td>\n      <td>PERSON</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>Woodhouse</td>\n      <td>woodhouse</td>\n      <td>Woodhouse</td>\n      <td>PROPN</td>\n      <td>PERSON</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>,</td>\n      <td>,</td>\n      <td>,</td>\n      <td>PUNCT</td>\n      <td></td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>handsome</td>\n      <td>handsome</td>\n      <td>handsome</td>\n      <td>ADJ</td>\n      <td></td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>,</td>\n      <td>,</td>\n      <td>,</td>\n      <td>PUNCT</td>\n      <td></td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>clever</td>\n      <td>clever</td>\n      <td>clever</td>\n      <td>ADJ</td>\n      <td></td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>,</td>\n      <td>,</td>\n      <td>,</td>\n      <td>PUNCT</td>\n      <td></td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>and</td>\n      <td>and</td>\n      <td>and</td>\n      <td>CCONJ</td>\n      <td></td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>rich</td>\n      <td>rich</td>\n      <td>rich</td>\n      <td>ADJ</td>\n      <td></td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>,</td>\n      <td>,</td>\n      <td>,</td>\n      <td>PUNCT</td>\n      <td></td>\n    </tr>\n    <tr>\n      <th>10</th>\n      <td>with</td>\n      <td>with</td>\n      <td>with</td>\n      <td>ADP</td>\n      <td></td>\n    </tr>\n    <tr>\n      <th>11</th>\n      <td>a</td>\n      <td>a</td>\n      <td>a</td>\n      <td>DET</td>\n      <td></td>\n    </tr>\n    <tr>\n      <th>12</th>\n      <td>comfortable</td>\n      <td>comfortable</td>\n      <td>comfortable</td>\n      <td>ADJ</td>\n      <td></td>\n    </tr>\n    <tr>\n      <th>13</th>\n      <td>home</td>\n      <td>home</td>\n      <td>home</td>\n      <td>NOUN</td>\n      <td></td>\n    </tr>\n    <tr>\n      <th>14</th>\n      <td>and</td>\n      <td>and</td>\n      <td>and</td>\n      <td>CCONJ</td>\n      <td></td>\n    </tr>\n    <tr>\n      <th>15</th>\n      <td>happy</td>\n      <td>happy</td>\n      <td>happy</td>\n      <td>ADJ</td>\n      <td></td>\n    </tr>\n    <tr>\n      <th>16</th>\n      <td>disposition</td>\n      <td>disposition</td>\n      <td>disposition</td>\n      <td>NOUN</td>\n      <td></td>\n    </tr>\n    <tr>\n      <th>17</th>\n      <td>,</td>\n      <td>,</td>\n      <td>,</td>\n      <td>PUNCT</td>\n      <td></td>\n    </tr>\n    <tr>\n      <th>18</th>\n      <td>seemed</td>\n      <td>seemed</td>\n      <td>seem</td>\n      <td>VERB</td>\n      <td></td>\n    </tr>\n    <tr>\n      <th>19</th>\n      <td>to</td>\n      <td>to</td>\n      <td>to</td>\n      <td>PART</td>\n      <td></td>\n    </tr>\n    <tr>\n      <th>20</th>\n      <td>unite</td>\n      <td>unite</td>\n      <td>unite</td>\n      <td>VERB</td>\n      <td></td>\n    </tr>\n    <tr>\n      <th>21</th>\n      <td>some</td>\n      <td>some</td>\n      <td>some</td>\n      <td>DET</td>\n      <td></td>\n    </tr>\n    <tr>\n      <th>22</th>\n      <td>of</td>\n      <td>of</td>\n      <td>of</td>\n      <td>ADP</td>\n      <td></td>\n    </tr>\n    <tr>\n      <th>23</th>\n      <td>the</td>\n      <td>the</td>\n      <td>the</td>\n      <td>DET</td>\n      <td></td>\n    </tr>\n    <tr>\n      <th>24</th>\n      <td>best</td>\n      <td>best</td>\n      <td>good</td>\n      <td>ADJ</td>\n      <td></td>\n    </tr>\n    <tr>\n      <th>25</th>\n      <td>blessings</td>\n      <td>blessings</td>\n      <td>blessing</td>\n      <td>NOUN</td>\n      <td></td>\n    </tr>\n    <tr>\n      <th>26</th>\n      <td>of</td>\n      <td>of</td>\n      <td>of</td>\n      <td>ADP</td>\n      <td></td>\n    </tr>\n    <tr>\n      <th>27</th>\n      <td>existence</td>\n      <td>existence</td>\n      <td>existence</td>\n      <td>NOUN</td>\n      <td></td>\n    </tr>\n    <tr>\n      <th>28</th>\n      <td>;</td>\n      <td>;</td>\n      <td>;</td>\n      <td>PUNCT</td>\n      <td></td>\n    </tr>\n    <tr>\n      <th>29</th>\n      <td>and</td>\n      <td>and</td>\n      <td>and</td>\n      <td>CCONJ</td>\n      <td></td>\n    </tr>\n    <tr>\n      <th>30</th>\n      <td>had</td>\n      <td>had</td>\n      <td>have</td>\n      <td>AUX</td>\n      <td></td>\n    </tr>\n    <tr>\n      <th>31</th>\n      <td>lived</td>\n      <td>lived</td>\n      <td>live</td>\n      <td>VERB</td>\n      <td></td>\n    </tr>\n    <tr>\n      <th>32</th>\n      <td>nearly</td>\n      <td>nearly</td>\n      <td>nearly</td>\n      <td>ADV</td>\n      <td>DATE</td>\n    </tr>\n    <tr>\n      <th>33</th>\n      <td>twenty</td>\n      <td>twenty</td>\n      <td>twenty</td>\n      <td>NUM</td>\n      <td>DATE</td>\n    </tr>\n    <tr>\n      <th>34</th>\n      <td>-</td>\n      <td>-</td>\n      <td>-</td>\n      <td>PUNCT</td>\n      <td>DATE</td>\n    </tr>\n    <tr>\n      <th>35</th>\n      <td>one</td>\n      <td>one</td>\n      <td>one</td>\n      <td>NUM</td>\n      <td>DATE</td>\n    </tr>\n    <tr>\n      <th>36</th>\n      <td>years</td>\n      <td>years</td>\n      <td>year</td>\n      <td>NOUN</td>\n      <td>DATE</td>\n    </tr>\n    <tr>\n      <th>37</th>\n      <td>in</td>\n      <td>in</td>\n      <td>in</td>\n      <td>ADP</td>\n      <td></td>\n    </tr>\n    <tr>\n      <th>38</th>\n      <td>the</td>\n      <td>the</td>\n      <td>the</td>\n      <td>DET</td>\n      <td></td>\n    </tr>\n    <tr>\n      <th>39</th>\n      <td>world</td>\n      <td>world</td>\n      <td>world</td>\n      <td>NOUN</td>\n      <td></td>\n    </tr>\n    <tr>\n      <th>40</th>\n      <td>with</td>\n      <td>with</td>\n      <td>with</td>\n      <td>ADP</td>\n      <td></td>\n    </tr>\n    <tr>\n      <th>41</th>\n      <td>very</td>\n      <td>very</td>\n      <td>very</td>\n      <td>ADV</td>\n      <td></td>\n    </tr>\n    <tr>\n      <th>42</th>\n      <td>little</td>\n      <td>little</td>\n      <td>little</td>\n      <td>ADJ</td>\n      <td></td>\n    </tr>\n    <tr>\n      <th>43</th>\n      <td>to</td>\n      <td>to</td>\n      <td>to</td>\n      <td>PART</td>\n      <td></td>\n    </tr>\n    <tr>\n      <th>44</th>\n      <td>distress</td>\n      <td>distress</td>\n      <td>distress</td>\n      <td>VERB</td>\n      <td></td>\n    </tr>\n    <tr>\n      <th>45</th>\n      <td>or</td>\n      <td>or</td>\n      <td>or</td>\n      <td>CCONJ</td>\n      <td></td>\n    </tr>\n    <tr>\n      <th>46</th>\n      <td>vex</td>\n      <td>vex</td>\n      <td>vex</td>\n      <td>VERB</td>\n      <td></td>\n    </tr>\n    <tr>\n      <th>47</th>\n      <td>her</td>\n      <td>her</td>\n      <td>-PRON-</td>\n      <td>PRON</td>\n      <td></td>\n    </tr>\n    <tr>\n      <th>48</th>\n      <td>.</td>\n      <td>.</td>\n      <td>.</td>\n      <td>PUNCT</td>\n      <td></td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def display_sent(asent):\n",
    "    headings=[\"token\",\"lower\",\"lemma\",\"pos\",\"NER\"]\n",
    "    info=[]\n",
    "    for t in asent:\n",
    "        info.append([t.text,t.lower_,t.lemma_,t.pos_,t.ent_type_])\n",
    "    return(pd.DataFrame(info,columns=headings))\n",
    "        \n",
    "display_sent(emma_sents[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1.1\n",
    "Run the `display_sent()` function on each of the first ten sentences of Emma (as stored in `emma_sents`).\n",
    "* What errors do you see in the named entity recognition?\n",
    "* Can you see any patterns in the words, lemmas or part-of-speech tags which might be used to improve the named entity recognition on these sentences?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1.2\n",
    "Write a function 'make_tag_lists()' which takes a list of sentences as input and which returns 3 lists:\n",
    "1. tokens\n",
    "2. POS tags\n",
    "3. Named Entity tags\n",
    "\n",
    "These lists should be the same length (189069 if applied to the all of the sentences in `nlp_emma`) and maintain the order of the text, i.e., position i in each list should refer to the same token."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tokens' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "\u001B[1;32m<ipython-input-16-07a2a48e9d12>\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[0;32m      4\u001B[0m     \u001B[1;32mreturn\u001B[0m \u001B[0mtokens\u001B[0m\u001B[1;33m,\u001B[0m\u001B[0mpos_tags\u001B[0m\u001B[1;33m,\u001B[0m\u001B[0mner_tags\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m      5\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m----> 6\u001B[1;33m \u001B[0mtoks\u001B[0m\u001B[1;33m,\u001B[0m\u001B[0mpos\u001B[0m\u001B[1;33m,\u001B[0m\u001B[0mner\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mmake_tag_lists\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0memma_sents\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m      7\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m<ipython-input-16-07a2a48e9d12>\u001B[0m in \u001B[0;36mmake_tag_lists\u001B[1;34m(sents)\u001B[0m\n\u001B[0;32m      2\u001B[0m     \u001B[1;31m### COMPLETE THE FUNCTIONALITY FOR MAKE_TAG_LISTS HERE\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m      3\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m----> 4\u001B[1;33m     \u001B[1;32mreturn\u001B[0m \u001B[0mtokens\u001B[0m\u001B[1;33m,\u001B[0m\u001B[0mpos_tags\u001B[0m\u001B[1;33m,\u001B[0m\u001B[0mner_tags\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m      5\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m      6\u001B[0m \u001B[0mtoks\u001B[0m\u001B[1;33m,\u001B[0m\u001B[0mpos\u001B[0m\u001B[1;33m,\u001B[0m\u001B[0mner\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mmake_tag_lists\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0memma_sents\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;31mNameError\u001B[0m: name 'tokens' is not defined"
     ]
    }
   ],
   "source": [
    "def make_tag_lists(sents):\n",
    "    ### COMPLETE THE FUNCTIONALITY FOR MAKE_TAG_LISTS HERE\n",
    "    \n",
    "    return tokens,pos_tags,ner_tags\n",
    "\n",
    "toks,pos,ner=make_tag_lists(emma_sents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "print(len(toks),len(pos),len(ner))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1.3\n",
    "Write a function which takes a list of tokens, a list of tags and a tag-type and returns a dictionary of all of the **chunks** which have the given tag-type; together with their frequency in the text.\n",
    "\n",
    "You can assume that two consecutive tokens with the same tag are part of the same chunk.\n",
    "\n",
    "Test your code and you should get the following output (for the given input):\n",
    "\n",
    "<img src=output-13.png>\n",
    "\n",
    "This tells us that \"Anne Cox\" is tagged twice as a named entity of type \"PERSON\" in the text.  How many occurrences of \"Miss Woodhouse\" tagged as a \"PERSON\" are there?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1.4\n",
    "Use your code to find \n",
    "* the 20 most commonly referred to people in Emma\n",
    "* the 20 most commonly referred to places in Emma (you will need to think about what tag(s) to use to find places)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1.5\n",
    "Look at the lists of people and places generated.  Assuming no knowledge of the characters or plot of Emma, what errors can you see?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extensions\n",
    "\n",
    "Code one or more of the following extensions.  In all cases, compare the lists of most frequently occurring named entities generated with the original ones.\n",
    "\n",
    "### Expanding NER Chunks\n",
    "* if the word immediately before or after a named entity chunk is POS-tagged as a PROPN, assume that this word is also part of the named entity chunk\n",
    "\n",
    "For example, where the token \"Miss\" has pos-tag \"PROPN\" and is immediately followed by a token labelled with \"PERSON\", then it should also be labelled with \"PERSON\". \n",
    "\n",
    "### Relabelling NER Chunks\n",
    "* if a named entity occurs more frequently elsewhere in the text as a different type, assume that it has been mis-labelled here\n",
    "\n",
    "For example, all 9 occurrences of \"Jane Fairfax\" labelled as \"GPE\" could be relabelled as \"PERSON\".\n",
    "\n",
    "### Linking NEs\n",
    "* find candidates for named entity linking.  \n",
    "\n",
    "For example, \"Churchill\" and \"Frank Churchill\" and \"Frank\" might all refer to the same person.\n",
    "However, you should proceed with care.  Anyone who knows the story well would tell you that \"Knightley\" and \"John Knightley\" do not refer to the same character (they are brothers).  As a further extension, give your linking functionality access to a list of known characters e.g., from https://www.janeausten.org/emma/cast-of-characters.asp\n",
    "\n",
    "### Co-occurring NEs\n",
    "* find NEs that tend to co-occur together.\n",
    "\n",
    "Can you find pairs of named entities which often occur together (or even better, occur more often together than one would expect if named entities occurred independently)?  You could consider pairs of people or alternatively co-occurrences of people and places.\n",
    "\n",
    "### NEs over Time\n",
    "* record the position in the text of each named entity occurrence\n",
    "* make a plot showing how the amount of occurrences of a given named entity varies with position in the text\n",
    "\n",
    "If you store each text position in `list_of_indices`, you could use:\n",
    "`pd.Series(np.histogram(list_of_indices, bins=num_bins)` to help you with this\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}